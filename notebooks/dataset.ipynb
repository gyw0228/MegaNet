{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "\n",
    "import resnet_v2 as resnet\n",
    "# import cv2\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.96s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "def get_data(base_dir,image_dir,ann_file):\n",
    "    image_path = '{}/images/{}'.format(baseDir,image_dir)\n",
    "    ann_path='{}/annotations/{}.json'.format(baseDir,ann_file)\n",
    "\n",
    "    return image_path, ann_path\n",
    "    \n",
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "imageTrainDir = 'train2014'\n",
    "imageValDir = 'val2014'\n",
    "imageTestDir = 'test2015'\n",
    "\n",
    "train_img_path, train_ann_path = get_data(baseDir,imageTrainDir,trainData)\n",
    "val_img_path, val_ann_path = get_data(baseDir,imageValDir,valData)\n",
    "\n",
    "# initialize a coco object\n",
    "coco = COCO(train_ann_path)\n",
    "\n",
    "# get all images containing the 'person' category\n",
    "catIds = coco.getCatIds(catNms=['person'])\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "catIds = imgIds[0:30]\n",
    "imgIds = imgIds[0:30]\n",
    "    \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    VGG_MEAN = tf.reshape(tf.constant([123.68, 116.78, 103.94]),[1,1,3])\n",
    "    \n",
    "    def extract_annotations(filename, imgID, coco=coco):\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "        ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "        bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "        keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "        mask = coco.annToMask(ann)\n",
    "\n",
    "        return filename, bbox, keypoints, mask\n",
    "    \n",
    "    \n",
    "    def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D = tf.constant(224.0)):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "        mask = tf.transpose([mask],[1,2,0])\n",
    "        bbox_tensor = tf.to_float(bbox_tensor)\n",
    "        keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "        sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "        centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "        centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "        center = tf.stack([centerX,centerY])\n",
    "\n",
    "        corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                             tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                             tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        i_shape = tf.subtract(corner2,corner1)\n",
    "        d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "        scale = tf.divide(D, sideLength)\n",
    "\n",
    "        cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                      tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "        cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                      tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "        dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "        dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "        pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "        pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "        pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "        pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "        # set invalid pts to 0\n",
    "        inbounds = tf.less(pts,D)\n",
    "        inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "        pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "\n",
    "        padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "        padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "        resized_image = tf.image.resize_images(padded_image,tf.constant([224,224]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        resized_image = resized_image - VGG_MEAN\n",
    "\n",
    "        resized_mask = tf.image.resize_images(padded_mask,tf.constant([224,224]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        return resized_image , resized_mask, pts, labels\n",
    "\n",
    "\n",
    "    # Initialize train_dataset\n",
    "    filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path,imgID) for imgID in imgIds])\n",
    "    imgID_tensor = tf.constant(imgIds)\n",
    "    \n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames,imgID_tensor))\n",
    "    # Extract Annotations via coco interface\n",
    "    train_dataset = train_dataset.map(lambda filename, imgID: tf.py_func(extract_annotations, [filename, imgID], \n",
    "                                                                 [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    # All other preprocessing in tensorflow\n",
    "    train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "    \n",
    "    # BATCH\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.batch(10) # must resize images to make them match\n",
    "    iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "    # resized_image, resized_mask, pts, labels = iterator.get_next()\n",
    "    images, masks, pts, labels = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "\n",
    "    \n",
    "    # Define model\n",
    "    net, endpoints = resnet.resnet_v2_50(images,\n",
    "                 num_classes=10,\n",
    "                 is_training=True,\n",
    "                 global_pool=False, # dense prediction\n",
    "                 output_stride=None,\n",
    "                 reuse=None,\n",
    "                 scope='resnet_v2_50')\n",
    "    \n",
    "    with tf.variable_scope(\"KyleScope\"):\n",
    "        branch_layer = endpoints['resnet_v2_50/block4/unit_1/bottleneck_v2/conv1']\n",
    "        with tf.variable_scope(\"level_1_Kyle\"):\n",
    "            branch_layer = tf.layers.conv2d(branch_layer,10,(3,3),(1,1),padding='same',activation=tf.nn.relu,name=\"Conv1\")\n",
    "            branch_layer = tf.layers.conv2d(branch_layer,10,(3,3),(1,1),padding='same',activation=tf.nn.relu,name=\"Conv2\")\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        file_writer = tf.summary.FileWriter('.', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(train_init_op) \n",
    "        sess.run(net)\n",
    "        sess.run(branch_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(base_dir,image_dir,ann_file):\n",
    "    image_path = '{}/images/{}'.format(baseDir,image_dir)\n",
    "    ann_path='{}/annotations/{}.json'.format(baseDir,ann_file)\n",
    "    \n",
    "    return image_path, ann_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "imageTrainDir = 'train2014'\n",
    "imageValDir = 'val2014'\n",
    "imageTestDir = 'test2015'\n",
    "\n",
    "train_img_path, train_ann_path = get_data(baseDir,imageTrainDir,trainData)\n",
    "val_img_path, val_ann_path = get_data(baseDir,imageValDir,valData)\n",
    "\n",
    "# initialize a coco object\n",
    "coco = COCO(train_ann_path)\n",
    "\n",
    "# get all images containing the 'person' category\n",
    "catIds = coco.getCatIds(catNms=['person'])\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "catIds = imgIds[0:30]\n",
    "imgIds = imgIds[0:30]\n",
    "\n",
    "# Initialize train_dataset\n",
    "\n",
    "filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path,imgID) for imgID in imgIds])\n",
    "imgID_tensor = tf.constant(imgIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_annotations(filename, imgID, coco=coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "    \n",
    "    return filename, bbox, keypoints, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D = tf.constant(224.0)):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "    mask = tf.transpose([mask],[1,2,0])\n",
    "    bbox_tensor = tf.to_float(bbox_tensor)\n",
    "    keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "    sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "    centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "    centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "    center = tf.stack([centerX,centerY])\n",
    "\n",
    "    corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    i_shape = tf.subtract(corner2,corner1)\n",
    "    d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "    scale = tf.divide(D, sideLength)\n",
    "\n",
    "    cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "    cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "    dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "    dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "    pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "    pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "    pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "    pts = tf.multiply(pts,scale) # scale keypoints\n",
    "    \n",
    "    # set invalid pts to 0\n",
    "    inbounds = tf.less(pts,D)\n",
    "    inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "    pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "\n",
    "    padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "    padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "    resized_image = tf.image.resize_images(padded_image,tf.constant([224,224]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_image = resized_image - VGG_MEAN\n",
    "    \n",
    "    resized_mask = tf.image.resize_images(padded_mask,tf.constant([224,224]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return resized_image , resized_mask, pts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames,imgID_tensor))\n",
    "# Extract Annotations via coco interface\n",
    "train_dataset = train_dataset.map(lambda filename, imgID: tf.py_func(extract_annotations, [filename, imgID], \n",
    "                                                             [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "# All other preprocessing in tensorflow\n",
    "train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BATCH\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(10) # must resize images to make them match\n",
    "\n",
    "iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                                   train_dataset.output_shapes)\n",
    "# resized_image, resized_mask, pts, labels = iterator.get_next()\n",
    "images, masks, pts, labels = iterator.get_next()\n",
    "\n",
    "train_init_op = iterator.make_initializer(train_dataset)\n",
    "# val_init_op = iterator.make_initializer(val_dataset)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Initialization Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool,name='is_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play around with ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    net, endpoints = resnet.resnet_v2_50(images,\n",
    "                     num_classes=10,\n",
    "                     is_training=True,\n",
    "                     global_pool=False, # dense prediction\n",
    "                     output_stride=None,\n",
    "                     reuse=None,\n",
    "                     scope='resnet_v2_50')\n",
    "    branch_layer = endpoints['resnet_v2_50/block4/unit_1/bottleneck_v2/conv1']\n",
    "    sess = tf.Session(graph=graph)\n",
    "    \n",
    "    file_writer = tf.summary.FileWriter('.', sess.graph)\n",
    "    sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "branch_layer = endpoints['resnet_v2_50/block4/unit_1/bottleneck_v2/conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "file_writer = tf.summary.FileWriter('.', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = tf.contrib.slim.nets.vgg\n",
    "with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=.9)):\n",
    "    logits, _ = vgg.vgg_16(images, num_classes=50, is_training=is_training,dropout_keep_prob=.5)\n",
    "    \n",
    "model_path = '/Users/kyle/Documents/SCHOOL/Stanford/STANFORD_2016_to_2017/3rd_quarter/CS231N/Project/ExampleCode/vgg_16.ckpt'\n",
    "assert(os.path.isfile(model_path))\n",
    "\n",
    "variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['vgg_16/fc8','vgg_16/fc7','vgg_16/fc6'])\n",
    "init_fn = tf.contrib.framework.assign_from_checkpoint_fn(model_path, variables_to_restore)\n",
    "variables_to_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_head = variables_to_restore[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = tf.layers.conv2d(vgg_head, filters=10,kernel_size=(3,3),strides=(1,1),padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Non frozen layer\n",
    "fc8_variables = tf.contrib.framework.get_variables('vgg_16/fc8')\n",
    "fc8_init = tf.variables_initializer(fc8_variables)\n",
    "fc8_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_fn(sess)  # load the pretrained weights\n",
    "# sess.run(fc8_init)  # initialize the new fc8 layer\n",
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resized_image, resized_mask, pts, labels = sess.run(next_element)\n",
    "\n",
    "try:\n",
    "    I, M, P, L = sess.run([images, masks, pts, labels], {is_training: True})\n",
    "    plt.imshow(I[0])\n",
    "    plt.imshow(M[0][:,:,0],alpha=0.5)\n",
    "    plt.scatter(P[0][(np.reshape(L[0],-1)==2),0],P[0][(np.reshape(L[0],-1)==2),1],c=\"r\")\n",
    "except tf.errors.OutOfRangeError:\n",
    "    sess.run(train_init_op)\n",
    "    print(\"Reinitialized Dataset Iterator...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
