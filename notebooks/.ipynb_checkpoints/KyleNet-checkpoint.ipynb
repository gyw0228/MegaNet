{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "\n",
    "import resnet_v2 as resnet\n",
    "# import cv2\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keypoint_CrossEntropyLoss(prediction_maps, keypoint_masks, labels, L=5.0, scope=\"keypointLoss\"):\n",
    "    \"\"\"\n",
    "    heat_maps = predictions from network\n",
    "    keypoints (N,17,2) = actual keypoint locations\n",
    "    labels (N,17,1) = 0 if invalid, 1 if occluded, 2 if valid\n",
    "    \"\"\"\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=prediction_maps,labels=keypoint_masks)\n",
    "    labels = tf.reshape(labels,[-1,1,1,17])\n",
    "    losses = tf.multiply(losses,labels) # set loss to zero for invalid keypoints (labels=0)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keypoint_SquaredErrorLoss(prediction_maps, keypoint_masks, labels, L=5.0, scope=\"keypointLoss\"):\n",
    "    \"\"\"\n",
    "    heat_maps = predictions from network\n",
    "    keypoints (N,17,2) = actual keypoint locations\n",
    "    labels (N,17,1) = 0 if invalid, 1 if occluded, 2 if valid\n",
    "    \"\"\"\n",
    "    losses = tf.squared_difference(prediction_maps,keypoint_masks)\n",
    "    labels = tf.reshape(labels,[-1,1,1,17])\n",
    "    losses = tf.multiply(losses,labels) # set loss to zero for invalid keypoints (labels=0)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_data(base_dir,image_dir,ann_file):\n",
    "    image_path = '{}/images/{}'.format(baseDir,image_dir)\n",
    "    ann_path='{}/annotations/{}.json'.format(baseDir,ann_file)\n",
    "\n",
    "    return image_path, ann_path\n",
    "    \n",
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "imageTrainDir = 'train2014'\n",
    "imageValDir = 'val2014'\n",
    "imageTestDir = 'test2015'\n",
    "\n",
    "train_img_path, train_ann_path = get_data(baseDir,imageTrainDir,trainData)\n",
    "val_img_path, val_ann_path = get_data(baseDir,imageValDir,valData)\n",
    "\n",
    "# initialize a coco object\n",
    "coco = COCO(train_ann_path)\n",
    "\n",
    "# get all images containing the 'person' category\n",
    "catIds = coco.getCatIds(catNms=['person'])\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "catIds = imgIds[0:30]\n",
    "imgIds = imgIds[0:30]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    VGG_MEAN = tf.reshape(tf.constant([123.68, 116.78, 103.94]),[1,1,3])\n",
    "    NUM_KEYPOINTS = 17\n",
    "    BATCH_SIZE = 10\n",
    "    L = 10.0 # keypoint effective radius\n",
    "    \n",
    "    def extract_annotations(filename, imgID, coco=coco):\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "        ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "        bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "        keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "        mask = coco.annToMask(ann)\n",
    "        \n",
    "        return filename, bbox, keypoints, mask\n",
    "    \n",
    "    def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D = tf.constant(256.0)):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        resized_image (N,D,D,3) - cropped, padded (if needed), scaled to square image of size D\n",
    "        resized_mask (N,D,D,1) - cropped, padded (if needed), scaled to square mask of size D\n",
    "        pts (N,2,17) - keypoint coordinates (i,j) scaled to match up with resized_image\n",
    "        labels (N,1,17) - values corresponding to pts: {0: invalid, 1:occluded, 2:valid}\n",
    "        \"\"\"\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "        mask = tf.transpose([mask],[1,2,0])\n",
    "        bbox_tensor = tf.to_float(bbox_tensor)\n",
    "        keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "        sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "        centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "        centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "        center = tf.stack([centerX,centerY])\n",
    "\n",
    "        corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                             tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                             tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        i_shape = tf.subtract(corner2,corner1)\n",
    "        d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "        scale = tf.divide(D, sideLength)\n",
    "        cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                      tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "        cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                      tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "        dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "        dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "        pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "        pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "        pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "        pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "        # set invalid pts to 0\n",
    "        inbounds = tf.less(pts,D)\n",
    "        inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "        pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "        pts = tf.transpose(pts,[1,0])\n",
    "        labels = tf.transpose(labels,[1,0])\n",
    "\n",
    "        padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "        padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "        resized_image = tf.image.resize_images(padded_image,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        resized_image = resized_image - VGG_MEAN\n",
    "        resized_mask = tf.image.resize_images(padded_mask,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        return resized_image, resized_mask, pts, labels\n",
    "\n",
    "    def scaleDownMaskAndKeypoints(image, mask, pts, labels):\n",
    "        mask = tf.image.resize_images(mask,tf.constant([128,128]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        pts = tf.multiply(pts,tf.constant(0.5))\n",
    "        return image, mask, pts, labels\n",
    "    \n",
    "    def generate_keypoint_masks(image, mask, keypoints, labels, D=128.0, L=L):\n",
    "        X, Y = tf.meshgrid(tf.linspace(0.0,128.0,128),tf.linspace(0.0,128.0,128))\n",
    "        X = tf.reshape(X,[128,128,1])\n",
    "        Y = tf.reshape(Y,[128,128,1])\n",
    "        X_stack = tf.tile(X,tf.constant([1,1,17],dtype=tf.int32))\n",
    "        Y_stack = tf.tile(Y,tf.constant([1,1,17],dtype=tf.int32))\n",
    "\n",
    "        pts = tf.reshape(keypoints,[1,2,17])\n",
    "        ptsX, ptsY = tf.split(pts,[1,1],axis=1)\n",
    "        d1 = tf.square(tf.subtract(X_stack,ptsX))\n",
    "        d2 = tf.square(tf.subtract(Y_stack,ptsY))\n",
    "\n",
    "        pt_masks = tf.multiply(tf.divide(tf.constant(1.0),tf.add(d1,d2)+L),L)\n",
    "        return image, mask, pt_masks, pts, labels\n",
    "    \n",
    "    ########## DATASET ###########\n",
    "    \n",
    "    with tf.variable_scope(\"DataSet\"):\n",
    "        # Initialize train_dataset\n",
    "        filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path,imgID) for imgID in imgIds])\n",
    "        imgID_tensor = tf.constant(imgIds)\n",
    "\n",
    "        train_dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames,imgID_tensor))\n",
    "        # Extract Annotations via coco interface\n",
    "        train_dataset = train_dataset.map(lambda filename, imgID: tf.py_func(extract_annotations, [filename, imgID], \n",
    "                                                                     [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "        # All other preprocessing in tensorflow\n",
    "        train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "        train_dataset = train_dataset.map(scaleDownMaskAndKeypoints)\n",
    "        train_dataset = train_dataset.map(generate_keypoint_masks)\n",
    "\n",
    "        # BATCH\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "        train_dataset = train_dataset.batch(10) # must resize images to make them match\n",
    "        iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "        # resized_image, resized_mask, pts, labels = iterator.get_next()\n",
    "#         images, masks, pts, labels = iterator.get_next()\n",
    "        images, masks, kpt_masks, pts, labels = iterator.get_next()\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        \n",
    "    \n",
    "    ##################################################################\n",
    "    ##################### BACKBONE ARCHITECTURE ######################\n",
    "    ##################################################################\n",
    "    \n",
    "    resnet_v2 = tf.contrib.slim.nets.resnet_v2\n",
    "    with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
    "        logits, endpoints = resnet_v2.resnet_v2_50(\n",
    "            inputs=images,\n",
    "            num_classes=10,\n",
    "            is_training=is_training,\n",
    "            reuse=None,\n",
    "            output_stride=16,\n",
    "            scope='resnet_v2_50'\n",
    "            )\n",
    "\n",
    "        model_path = 'checkpoints/resnet_v2_50.ckpt'\n",
    "        assert(os.path.isfile(model_path))\n",
    "        # Backbone Variables - remember to exclude all variables above backbone (including block4 and logits)\n",
    "        variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['resnet_v2_50/postnorm','resnet_v2_50/logits'])\n",
    "        # Head variables\n",
    "        # Note: We would need another set of variables and another initializer to capture the logits as well\n",
    "        other_variables = tf.contrib.framework.get_variables('resnet_v2_50/postnorm')\n",
    "\n",
    "    \n",
    "    ##################################################################\n",
    "    ####################### HEAD ARCHITECTURE ########################\n",
    "    ##################################################################\n",
    "    \n",
    "    HEAD_SCOPE = 'NetworkHead'\n",
    "    PREDICTION_THRESHOLD = 0.5\n",
    "\n",
    "    with tf.variable_scope(HEAD_SCOPE):\n",
    "        block1 = endpoints['resnet_v2_50/block1']\n",
    "        block2 = endpoints['resnet_v2_50/block2']\n",
    "        block3 = endpoints['resnet_v2_50/block3']\n",
    "        block4 = endpoints['resnet_v2_50/block4']\n",
    "\n",
    "        with tf.variable_scope('Layer1'):\n",
    "            b1 = tf.layers.conv2d(block1, 64, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "            b2 = tf.layers.conv2d(block2, 128, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "            b3 = tf.layers.conv2d(block3, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "            b4 = tf.layers.conv2d(block4, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope('Layer2'):\n",
    "            b1 = tf.layers.conv2d(block1, 32, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "\n",
    "            b2 = tf.layers.conv2d_transpose(b2, 32, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "            b3 = tf.layers.conv2d_transpose(b3, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "            b4 = tf.layers.conv2d_transpose(b4, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "\n",
    "            b2 = tf.slice(b2,[0,1,1,0],b2.shape - np.array([0, 2, 2, 0]))\n",
    "            b3 = tf.slice(b3,[0,1,1,0],b3.shape - np.array([0, 2, 2, 0]))\n",
    "            b4 = tf.slice(b4,[0,1,1,0],b4.shape - np.array([0, 2, 2, 0]))\n",
    "\n",
    "        with tf.variable_scope('BatchNorm'):\n",
    "            b1 = tf.layers.batch_normalization(b1)\n",
    "            b2 = tf.layers.batch_normalization(b2)\n",
    "            b3 = tf.layers.batch_normalization(b3)\n",
    "            b4 = tf.layers.batch_normalization(b4)\n",
    "\n",
    "        with tf.variable_scope('Funnel'):\n",
    "            head = tf.concat([b1,b2,b3,b4],axis=3)\n",
    "\n",
    "        with tf.variable_scope('MaskHead'):\n",
    "            mask_head = tf.layers.conv2d_transpose(head, 16, (3,3), (2,2), padding='VALID', activation=tf.nn.relu)\n",
    "            mask_head = tf.slice(mask_head,[0,1,1,0],mask_head.shape - np.array([0, 2, 2, 0]))\n",
    "            mask_head = tf.layers.conv2d(mask_head, 1, (1,1), (1,1), padding='SAME', activation=None)\n",
    "\n",
    "        with tf.variable_scope('KeypointHead'):\n",
    "            keypoint_head = tf.layers.conv2d_transpose(head, 32, (3,3), (2,2), padding='VALID', activation = tf.nn.relu)\n",
    "            keypoint_head = tf.slice(keypoint_head,[0,1,1,0],keypoint_head.shape - np.array([0, 2, 2, 0]))\n",
    "            keypoint_head = tf.layers.conv2d(keypoint_head, 17, (1,1), (1,1), padding='SAME', activation=None)\n",
    "\n",
    "        with tf.variable_scope('MaskPrediction'):\n",
    "            mask_prediction = tf.nn.sigmoid(mask_head)\n",
    "            mask_prediction = tf.to_float(tf.greater_equal(mask_prediction, PREDICTION_THRESHOLD))\n",
    "\n",
    "        with tf.variable_scope('KeypointsPrediction'):\n",
    "            keypoint_prediction = tf.nn.sigmoid(keypoint_head)\n",
    "            keypoint_prediction = tf.to_float(tf.greater_equal(keypoint_prediction, PREDICTION_THRESHOLD))\n",
    "\n",
    "            \n",
    "            \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        file_writer = tf.summary.FileWriter('/tmp/HourGlassNet/1')\n",
    "        file_writer.add_graph(sess.graph)\n",
    "        \n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # initialize dataset\n",
    "        sess.run(train_init_op) \n",
    "        masks, kpt_masks, mask_pred, kpt_pred, mask_loss, kpt_loss = sess.run([masks, kpt_masks, maskPrediction, \n",
    "                                                             keypointPredictions, maskLoss, keypointLoss])\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(mask_pred[i][:,:,0])\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(kpt_pred[i][:,:,0])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(masks[i][:,:,0])\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(np.sum(kpt_masks[i],axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_fn(sess)  # load the pretrained weights\n",
    "# sess.run(fc8_init)  # initialize the new fc8 layer\n",
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resized_image, resized_mask, pts, labels = sess.run(next_element)\n",
    "\n",
    "try:\n",
    "    I, M, P, L = sess.run([images, masks, pts, labels], {is_training: True})\n",
    "    plt.imshow(I[0])\n",
    "    plt.imshow(M[0][:,:,0],alpha=0.5)\n",
    "    plt.scatter(P[0][(np.reshape(L[0],-1)==2),0],P[0][(np.reshape(L[0],-1)==2),1],c=\"r\")\n",
    "except tf.errors.OutOfRangeError:\n",
    "    sess.run(train_init_op)\n",
    "    print(\"Reinitialized Dataset Iterator...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Playing around with ResNet V2 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "D = 225\n",
    "images = tf.placeholder(tf.float32, (BATCH_SIZE,D,D,3))\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "resnet_v2 = tf.contrib.slim.nets.resnet_v2\n",
    "with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
    "    logits, endpoints = resnet_v2.resnet_v2_50(\n",
    "        inputs=images,\n",
    "        num_classes=10,\n",
    "        is_training=is_training,\n",
    "        reuse=None,\n",
    "        output_stride=16,\n",
    "        scope='resnet_v2_50'\n",
    "        )\n",
    "\n",
    "# model_path = 'checkpoints/resnet_v2_50.ckpt'\n",
    "# assert(os.path.isfile(model_path))\n",
    "# # Backbone Variables - remember to exclude all variables above backbone (including block4 and logits)\n",
    "# variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['resnet_v2_50/block4','resnet_v2_50/postnorm','resnet_v2_50/logits'])\n",
    "# # Head variables\n",
    "# # Note: We would need another set of variables and another initializer to capture the logits as well\n",
    "# other_variables = tf.contrib.framework.get_variables('resnet_v2_50/block4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highestPrimeFactorization(n):    \n",
    "    return [(i, n//i) for i in range(1, int(n**0.5) + 1) if n % i == 0][-1]\n",
    "\n",
    "def getFilterImageSummary(filters,name=None):\n",
    "    padded_filters = tf.pad(filters,tf.constant([[0,0],[1,1],[1,1],[0,0],[0,0]]),'CONSTANT')\n",
    "    filter_list = tf.unstack(padded_filters,axis=4)    \n",
    "    H,W = highestPrimeFactorization(len(filter_list))\n",
    "    weight_strips = [tf.concat(filter_list[8*i:8*(i+1)],axis=1) for i in range(W)]\n",
    "    weight_image = tf.concat(weight_strips,axis=2)\n",
    "    return weight_image\n",
    "    \n",
    "def getActivationImageSummary(activations,name=None):\n",
    "    padded_activations = tf.pad(activations,tf.constant([[0,0],[1,0],[1,0],[0,0]]),'CONSTANT')\n",
    "    expanded_activations = tf.expand_dims(padded_activations,axis=3)\n",
    "    activations_list = tf.unstack(expanded_activations,axis=4)\n",
    "    H,W = highestPrimeFactorization(len(activations_list))\n",
    "    activation_strips = [tf.concat(activations_list[H*i:H*(i+1)],axis=1) for i in range(W)]\n",
    "    activation_image = tf.concat(activation_strips,axis=2)\n",
    "    return activation_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HEAD_SCOPE = 'NetworkHead'\n",
    "PREDICTION_THRESHOLD = 0.5\n",
    "\n",
    "with tf.variable_scope(HEAD_SCOPE):\n",
    "    block1 = endpoints['resnet_v2_50/block1']\n",
    "    block2 = endpoints['resnet_v2_50/block2']\n",
    "    block3 = endpoints['resnet_v2_50/block3']\n",
    "    block4 = endpoints['resnet_v2_50/block4']\n",
    "    \n",
    "    with tf.variable_scope('Layer1'):\n",
    "        b1 = tf.layers.conv2d(block1, 64, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "        b2 = tf.layers.conv2d(block2, 128, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "        b3 = tf.layers.conv2d(block3, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "        b4 = tf.layers.conv2d(block4, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "    \n",
    "    with tf.variable_scope('Layer2'):\n",
    "        b1 = tf.layers.conv2d(block1, 32, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "\n",
    "        b2 = tf.layers.conv2d_transpose(b2, 32, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "        b3 = tf.layers.conv2d_transpose(b3, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "        b4 = tf.layers.conv2d_transpose(b4, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "\n",
    "        b2 = tf.slice(b2,[0,1,1,0],b2.shape - np.array([0, 2, 2, 0]))\n",
    "        b3 = tf.slice(b3,[0,1,1,0],b3.shape - np.array([0, 2, 2, 0]))\n",
    "        b4 = tf.slice(b4,[0,1,1,0],b4.shape - np.array([0, 2, 2, 0]))\n",
    "    \n",
    "    with tf.variable_scope('BatchNorm'):\n",
    "        b1 = tf.layers.batch_normalization(b1)\n",
    "        b2 = tf.layers.batch_normalization(b2)\n",
    "        b3 = tf.layers.batch_normalization(b3)\n",
    "        b4 = tf.layers.batch_normalization(b4)\n",
    "        \n",
    "    with tf.variable_scope('Funnel'):\n",
    "        head = tf.concat([b1,b2,b3,b4],axis=3)\n",
    "\n",
    "    with tf.variable_scope('MaskHead'):\n",
    "        mask_head = tf.layers.conv2d_transpose(head, 16, (3,3), (2,2), padding='VALID', activation=tf.nn.relu)\n",
    "        mask_head = tf.slice(mask_head,[0,1,1,0],mask_head.shape - np.array([0, 2, 2, 0]))\n",
    "        mask_head = tf.layers.conv2d(mask_head, 1, (1,1), (1,1), padding='SAME', activation=None)\n",
    "        \n",
    "    with tf.variable_scope('KeypointHead'):\n",
    "        keypoint_head = tf.layers.conv2d_transpose(head, 32, (3,3), (2,2), padding='VALID', activation = tf.nn.relu)\n",
    "        keypoint_head = tf.slice(keypoint_head,[0,1,1,0],keypoint_head.shape - np.array([0, 2, 2, 0]))\n",
    "        keypoint_head = tf.layers.conv2d(keypoint_head, 17, (1,1), (1,1), padding='SAME', activation=None)\n",
    "        \n",
    "    with tf.variable_scope('MaskPrediction'):\n",
    "        mask_prediction = tf.nn.sigmoid(mask_head)\n",
    "        mask_prediction = tf.to_float(tf.greater_equal(mask_prediction, PREDICTION_THRESHOLD))\n",
    "        \n",
    "    with tf.variable_scope('KeypointsPrediction'):\n",
    "        keypoint_prediction = tf.nn.sigmoid(keypoint_head)\n",
    "        keypoint_prediction = tf.to_float(tf.greater_equal(keypoint_prediction, PREDICTION_THRESHOLD))\n",
    "\n",
    "    \n",
    "mask_prediction.get_shape().as_list(), keypoint_prediction.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG_MEAN = tf.reshape(tf.constant([123.68, 116.78, 103.94]),[1,1,3])\n",
    "NUM_KEYPOINTS = 17\n",
    "BATCH_SIZE = 10\n",
    "L = 10.0 # keypoint effective radius\n",
    "\n",
    "def extract_annotations(filename, imgID, coco=coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "\n",
    "    return filename, bbox, keypoints, mask\n",
    "\n",
    "def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D = tf.constant(256.0)):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    resized_image (N,D,D,3) - cropped, padded (if needed), scaled to square image of size D\n",
    "    resized_mask (N,D,D,1) - cropped, padded (if needed), scaled to square mask of size D\n",
    "    pts (N,2,17) - keypoint coordinates (i,j) scaled to match up with resized_image\n",
    "    labels (N,1,17) - values corresponding to pts: {0: invalid, 1:occluded, 2:valid}\n",
    "    \"\"\"\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "    mask = tf.transpose([mask],[1,2,0])\n",
    "    bbox_tensor = tf.to_float(bbox_tensor)\n",
    "    keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "    sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "    centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "    centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "    center = tf.stack([centerX,centerY])\n",
    "\n",
    "    corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    i_shape = tf.subtract(corner2,corner1)\n",
    "    d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "    scale = tf.divide(D, sideLength)\n",
    "    cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "    cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "    dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "    dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "    pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "    pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "    pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "    pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "    # set invalid pts to 0\n",
    "    inbounds = tf.less(pts,D)\n",
    "    inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "    pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "    pts = tf.transpose(pts,[1,0])\n",
    "    labels = tf.transpose(labels,[1,0])\n",
    "\n",
    "    padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "    padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "    resized_image = tf.image.resize_images(padded_image,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_image = resized_image - VGG_MEAN\n",
    "    resized_mask = tf.image.resize_images(padded_mask,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return resized_image, resized_mask, pts, labels\n",
    "\n",
    "def scaleDownMaskAndKeypoints(image, mask, pts, labels):\n",
    "    mask = tf.image.resize_images(mask,tf.constant([128,128]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    pts = tf.multiply(pts,tf.constant(0.5))\n",
    "    return image, mask, pts, labels\n",
    "\n",
    "def generate_keypoint_masks(image, mask, keypoints, labels, D=128.0, L=L):\n",
    "    X, Y = tf.meshgrid(tf.linspace(0.0,128.0,128),tf.linspace(0.0,128.0,128))\n",
    "    X = tf.reshape(X,[128,128,1])\n",
    "    Y = tf.reshape(Y,[128,128,1])\n",
    "    X_stack = tf.tile(X,tf.constant([1,1,17],dtype=tf.int32))\n",
    "    Y_stack = tf.tile(Y,tf.constant([1,1,17],dtype=tf.int32))\n",
    "\n",
    "    pts = tf.reshape(keypoints,[1,2,17])\n",
    "    ptsX, ptsY = tf.split(pts,[1,1],axis=1)\n",
    "    d1 = tf.square(tf.subtract(X_stack,ptsX))\n",
    "    d2 = tf.square(tf.subtract(Y_stack,ptsY))\n",
    "\n",
    "    pt_masks = tf.multiply(tf.divide(tf.constant(1.0),tf.add(d1,d2)+L),L)\n",
    "    return image, mask, pt_masks, pts, labels\n",
    "\n",
    "########## DATASET ###########\n",
    "\n",
    "with tf.variable_scope(\"DataSet\"):\n",
    "    # Initialize train_dataset\n",
    "    filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path,imgID) for imgID in imgIds])\n",
    "    imgID_tensor = tf.constant(imgIds)\n",
    "\n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames,imgID_tensor))\n",
    "    # Extract Annotations via coco interface\n",
    "    train_dataset = train_dataset.map(lambda filename, imgID: tf.py_func(extract_annotations, [filename, imgID], \n",
    "                                                                 [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    # All other preprocessing in tensorflow\n",
    "    train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "    train_dataset = train_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    train_dataset = train_dataset.map(generate_keypoint_masks)\n",
    "\n",
    "    # BATCH\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.batch(10) # must resize images to make them match\n",
    "    iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "    # resized_image, resized_mask, pts, labels = iterator.get_next()\n",
    "#         images, masks, pts, labels = iterator.get_next()\n",
    "    images, masks, kpt_masks, pts, labels = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None, 256, 256, 3],\n",
       " [None, 128, 128, None],\n",
       " [None, 128, 128, 17],\n",
       " [None, 1, 2, 17],\n",
       " [None, None, None])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape.as_list(), masks.shape.as_list(), kpt_masks.shape.as_list(), pts.shape.as_list(), labels.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(17)]),\n",
       " TensorShape([Dimension(None), Dimension(17)]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 128\n",
    "x = tf.reshape(tf.linspace(0.5,d-0.5,d),[1,d,1,1])\n",
    "pred = tf.multiply(kpt_masks, tf.to_float(tf.greater_equal(kpt_masks,0.5)))\n",
    "pred_i = tf.reduce_sum(tf.multiply(pred, x),axis=[1,2])/tf.reduce_sum(pred,axis=[1,2])\n",
    "pred_j = tf.reduce_sum(tf.multiply(pred, tf.transpose(x,(0,2,1,3))),axis=[1,2])/tf.reduce_sum(pred,axis=[1,2])\n",
    "pred_i.shape,pred_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KeypointPrediction(pred_masks, d):\n",
    "    x = tf.reshape(tf.linspace(0.5,d-0.5,d),[1,d,1,1])\n",
    "    pred = tf.multiply(kpt_masks, tf.to_float(tf.greater_equal(kpt_masks,0.5)))\n",
    "    pred_i = tf.reduce_sum(tf.multiply(pred, x),axis=[1,2])/tf.reduce_sum(pred,axis=[1,2])\n",
    "    pred_j = tf.reduce_sum(tf.multiply(pred, tf.transpose(x,(0,2,1,3))),axis=[1,2])/tf.reduce_sum(pred,axis=[1,2])\n",
    "    pred_pts = tf.stack([pred_j,pred_i],axis=1)\n",
    "    return pred_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_pts = KeypointPrediction(kpt_masks, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pPts, kPts, kpts_masks_truth = sess.run([pred_pts, pts, kpt_masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  74.80831909,   79.08973694,   72.66459656,   88.15563202,\n",
       "           1.47866142,   96.22941589,   67.12387085,   98.99401855,\n",
       "          50.00239563,   87.13562012,   31.81023979,   90.26578522,\n",
       "          68.18241119,   82.84353638,   45.82157135,   80.16590118,\n",
       "          40.1860199 ],\n",
       "       [  22.58893967,   20.35419273,   20.34156799,   17.82577133,\n",
       "           1.4786613 ,   31.77056694,   30.0628109 ,   47.95923615,\n",
       "          40.9604187 ,   53.16421127,   49.49118423,   65.0824585 ,\n",
       "          61.86225128,   84.48488617,   74.83892822,  119.40825653,\n",
       "         109.15135956]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pPts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  74.84745789,   79.18643951,   72.67796326,   88.40677643,\n",
       "          -0.        ,   96.54237366,   67.25423431,   99.25423431,\n",
       "          49.89830399,   87.32203674,   31.45762634,   90.57627106,\n",
       "          68.33898163,   82.98304749,   45.55932236,   80.27118683,\n",
       "          40.13559341],\n",
       "       [  22.23728752,   20.06779671,   20.06779671,   17.35593224,\n",
       "          -0.        ,   31.45762634,   29.83050919,   47.72881317,\n",
       "          40.67796707,   53.15254211,   49.35593414,   65.08474731,\n",
       "          61.83050919,   84.61016846,   74.84745789,  119.8644104 ,\n",
       "         109.55931854]], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kPts[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12434ff98>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHVCAYAAABfWZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122798748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(kpts_masks_truth[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = np.meshgrid(np.linspace(1,128,128),np.linspace(1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xHat = np.sum(x*(kpts_masks_truth[0][:,:,0]))/np.sum(kpts_masks_truth[0][:,:,0])\n",
    "yHat = np.sum(y*(kpts_masks_truth[0][:,:,0]))/np.sum(kpts_masks_truth[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.92587"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(kpts_masks_truth[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66.656371667675742, 22.07448185339204)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xHat, yHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x125e062b0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHVCAYAAABfWZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1276270f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x*(kpts_masks_truth[0][:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
