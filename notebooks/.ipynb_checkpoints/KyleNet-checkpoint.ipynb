{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "\n",
    "import resnet_v2 as resnet\n",
    "# import cv2\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing COCO objects to extract training and validation datasets...\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=19.68s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=9.44s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_data(base_dir,image_dir,ann_file):\n",
    "    image_path = '{}/images/{}'.format(baseDir,image_dir)\n",
    "    ann_path='{}/annotations/{}.json'.format(baseDir,ann_file)\n",
    "\n",
    "    return image_path, ann_path\n",
    "    \n",
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "imageTrainDir = 'train2014'\n",
    "imageValDir = 'val2014'\n",
    "imageTestDir = 'test2015'\n",
    "\n",
    "train_img_path, train_ann_path = get_data(baseDir,imageTrainDir,trainData)\n",
    "val_img_path, val_ann_path = get_data(baseDir,imageValDir,valData)\n",
    "# initialize a coco object\n",
    "print(\"Initializing COCO objects to extract training and validation datasets...\\n\")\n",
    "train_coco = COCO(train_ann_path)\n",
    "val_coco = COCO(val_ann_path)\n",
    "# get all images containing the 'person' category\n",
    "train_catIds = train_coco.getCatIds(catNms=['person'])\n",
    "train_imgIds = train_coco.getImgIds(catIds=train_catIds)\n",
    "val_catIds = val_coco.getCatIds(catNms=['person'])\n",
    "val_imgIds = val_coco.getImgIds(catIds=val_catIds)\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "# if args.small_dataset:\n",
    "train_catIds = train_catIds[0:30]\n",
    "train_imgIds = train_imgIds[0:30]\n",
    "val_catIds = val_catIds[0:30]\n",
    "val_imgIds = val_imgIds[0:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a8e179509cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNUM_KEYPOINTS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5.0\u001b[0m \u001b[0;31m# keypoint effective radius\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;31m# image height and width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "############### VARIOUS HYPER-PARAMETERS ##############\n",
    "#######################################################\n",
    "\n",
    "NUM_KEYPOINTS = 17\n",
    "BATCH_SIZE = 10\n",
    "L = 5.0 # keypoint effective radius\n",
    "D = 256 # image height and width\n",
    "d = 128 # evaluation height and width (for mask and keypoint masks)\n",
    "MASK_THRESHOLD = 0.5 # threshold for on/off prediction (in mask and keypoint masks)\n",
    "KP_THRESHOLD = 0.5 # threshold for on/off prediction (in mask and keypoint masks)\n",
    "KP_VIS_THRESHOLD = 0.9 # Threshold for visualization of KP masks in tf image summaries\n",
    "KP_DISTANCE_THRESHOLD = 5.0 # threshold for determining if a keypoint estimate is accurate\n",
    "X_INIT = tf.contrib.layers.xavier_initializer_conv2d() # xavier initializer for head architecture\n",
    "learning_rate1 = args.learning_rate1\n",
    "\n",
    "#######################################################\n",
    "################## SUMMARY DICTIONARY #################\n",
    "#######################################################\n",
    "image_summary_list = []\n",
    "scalar_summary_list = []\n",
    "histogram_summary_list = []\n",
    "\n",
    "#######################################################\n",
    "################### PREPARE DATASET ###################\n",
    "#######################################################\n",
    "with tf.variable_scope('DataSet'):\n",
    "    print(\"Initializing Dataset...\\n\")\n",
    "    #######################################################\n",
    "    ##### PRE-PROCESSING AND DATASET EXTRACTION TOOLS #####\n",
    "    #######################################################\n",
    "    def extract_annotations_train(filename, imgID, coco=train_coco):\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "        ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "        bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "        keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "        mask = coco.annToMask(ann)\n",
    "\n",
    "        return filename, bbox, keypoints, mask\n",
    "\n",
    "    def extract_annotations_val(filename, imgID, coco=val_coco):\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "        ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "        bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "        keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "        mask = coco.annToMask(ann)\n",
    "\n",
    "        return filename, bbox, keypoints, mask\n",
    "\n",
    "    def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D=D):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        resized_image (N,D,D,3) - cropped, padded (if needed), scaled to square image of size D\n",
    "        resized_mask (N,D,D,1) - cropped, padded (if needed), scaled to square mask of size D\n",
    "        pts (N,2,17) - keypoint coordinates (i,j) scaled to match up with resized_image\n",
    "        labels (N,1,17) - values corresponding to pts: {0: invalid, 1:occluded, 2:valid}\n",
    "        \"\"\"\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "        # subtract mean\n",
    "        image = tf.subtract(image, tf.reduce_mean(image))\n",
    "\n",
    "        mask = tf.transpose([mask],[1,2,0])\n",
    "        bbox_tensor = tf.to_float(bbox_tensor)\n",
    "        keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "        sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "        centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "        centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "        center = tf.stack([centerX,centerY])\n",
    "\n",
    "        corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                            tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                            tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        i_shape = tf.subtract(corner2,corner1)\n",
    "        d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "        scale = tf.divide(tf.constant(D,tf.float32), sideLength)\n",
    "        cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                    tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "        cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                    tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "        dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "        dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "        pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "        pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "        pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "        pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "        # set invalid pts to 0\n",
    "        valid = tf.less(pts,tf.constant(D,tf.float32))\n",
    "        valid = tf.reduce_min(tf.multiply(tf.to_int32(valid), tf.to_int32(tf.greater(pts,0))), axis=0,keep_dims=True)\n",
    "        pts = tf.multiply(pts,tf.to_float(valid))\n",
    "        pts = tf.transpose(pts,[1,0])\n",
    "        labels = tf.transpose(labels,[1,0])\n",
    "        labels = tf.to_float(tf.greater_equal(labels, 2)) # only use labels whose values are 2 - is this a good idea?\n",
    "        # labels = labels * tf.reduce_min(tf.to_float(tf.transpose(valid,[1,0])), axis=0) # make sure no invalid labels are getting through\n",
    "\n",
    "        print(valid.shape)\n",
    "        print(pts.shape)\n",
    "        print(labels.shape)\n",
    "        \n",
    "        padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "        padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "        # if image size is not square, set labels to zero (so zero-padding won't affect training)\n",
    "        is_padded = tf.reduce_min(tf.to_float(tf.less(dX, 1.0)))\n",
    "        labels = is_padded * labels\n",
    "\n",
    "        resized_image = tf.image.resize_images(padded_image,tf.constant([D,D]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        # resized_image = resized_image - VGG_MEAN\n",
    "        resized_mask = tf.image.resize_images(padded_mask,tf.constant([D,D]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        return resized_image, resized_mask, pts, labels\n",
    "\n",
    "    def scaleDownMaskAndKeypoints(image, mask, pts, labels, d=d, D=D):\n",
    "        mask = tf.image.resize_images(mask,tf.constant([d,d]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        pts = tf.multiply(pts,tf.constant(d/D))\n",
    "        return image, mask, pts, labels\n",
    "\n",
    "    def generate_keypoint_masks(image, mask, keypoints, labels, d=d, D=D, L=L):\n",
    "        X, Y = tf.meshgrid(tf.linspace(0.0,d,d),tf.linspace(0.0,d,d))\n",
    "        X = tf.reshape(X,[d,d,1])\n",
    "        Y = tf.reshape(Y,[d,d,1])\n",
    "        X_stack = tf.tile(X,tf.constant([1,1,17],dtype=tf.int32))\n",
    "        Y_stack = tf.tile(Y,tf.constant([1,1,17],dtype=tf.int32))\n",
    "\n",
    "        pts = tf.reshape(keypoints,[1,2,17])\n",
    "        ptsX, ptsY = tf.split(pts,[1,1],axis=1)\n",
    "        d1 = tf.square(tf.subtract(X_stack,ptsX))\n",
    "        d2 = tf.square(tf.subtract(Y_stack,ptsY))\n",
    "\n",
    "        pt_masks = tf.multiply(tf.divide(tf.constant(1.0),tf.add(d1,d2)+L),L)\n",
    "        return image, mask, pt_masks, pts, labels\n",
    "    def generate_one_hot_keypoint_masks(image, mask, keypoints, labels, d=d):\n",
    "        pts = tf.reshape(keypoints,[1,2,17])\n",
    "        indices = tf.to_int32(pts)\n",
    "        kp_mask1 = tf.one_hot(depth=d,indices=indices[:,1,:],axis=0)\n",
    "        kp_mask2 = tf.one_hot(depth=d,indices=indices[:,0,:],axis=1)\n",
    "        kp_masks = tf.matmul(tf.transpose(kp_mask1,(2,0,1)),tf.transpose(kp_mask2,(2,0,1)))\n",
    "        kp_masks = tf.transpose(kp_masks,(1,2,0))\n",
    "        return image, mask, kp_masks, pts, labels\n",
    "\n",
    "\n",
    "    def softmax_keypoint_masks(kpt_masks, d=d):\n",
    "        return tf.reshape(tf.nn.softmax(tf.reshape(kpt_masks, [-1,d**2,17]),dim=1),[-1,d,d,17])\n",
    "    def bilinear_filter(channels_in,channels_out):\n",
    "        f = tf.multiply(tf.constant([0.5, 1.0, 0.5],shape=[3,1]),tf.constant([0.5, 1.0, 0.5],shape=[1,3]))\n",
    "        f = tf.stack([f for i in range(channels_out)],axis=2)\n",
    "        f = tf.stack([f for i in range(channels_in)],axis=3)\n",
    "        return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # get all images containing the 'person' category\n",
    "    train_catIds = train_coco.getCatIds(catNms=['person'])\n",
    "    train_imgIds = train_coco.getImgIds(catIds=train_catIds)\n",
    "    val_catIds = val_coco.getCatIds(catNms=['person'])\n",
    "    val_imgIds = val_coco.getImgIds(catIds=val_catIds)\n",
    "\n",
    "    # Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "    if args.small_dataset == True:\n",
    "        train_catIds = train_catIds[0:30]\n",
    "        train_imgIds = train_imgIds[0:30]\n",
    "        val_catIds = val_catIds[0:30]\n",
    "        val_imgIds = val_imgIds[0:30]\n",
    "\n",
    "    ################### TRAIN DATASET ###################\n",
    "    train_filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path, imgID) for imgID in train_imgIds])\n",
    "    train_imgID_tensor = tf.constant(train_imgIds)\n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((train_filenames, train_imgID_tensor))\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda filename, imgID: tf.py_func(extract_annotations_train, [filename, imgID], [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "    train_dataset = train_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    train_dataset = train_dataset.map(generate_one_hot_keypoint_masks)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    #################### VAL DATASET ####################\n",
    "    val_filenames = tf.constant(['{}/COCO_val2014_{:0>12}.jpg'.format(val_img_path, imgID) for imgID in val_imgIds])\n",
    "    val_imgID_tensor = tf.constant(val_imgIds)\n",
    "    val_dataset = tf.contrib.data.Dataset.from_tensor_slices((val_filenames, val_imgID_tensor))\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda filename, imgID: tf.py_func(extract_annotations_val,[filename, imgID],[filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    val_dataset = val_dataset.map(preprocess_image_tf)\n",
    "    val_dataset = val_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    val_dataset = val_dataset.map(generate_one_hot_keypoint_masks)\n",
    "    val_dataset = val_dataset.shuffle(buffer_size=10000)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "\n",
    "    images, masks, kpt_masks, pts, labels = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    val_init_op = iterator.make_initializer(val_dataset)\n",
    "\n",
    "    image_summary_list.append(tf.summary.image('keypoint masks', getActivationImage(kpt_masks),max_outputs=1))\n",
    "    image_summary_list.append(tf.summary.image('input images', images, max_outputs=1))\n",
    "    image_summary_list.append(tf.summary.image('keypoint_overlays', keypointHeatMapOverlay(images, kpt_masks, threshold=KP_VIS_THRESHOLD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_keypoint_masks(kpt_masks, d=56):\n",
    "    return tf.reshape(tf.nn.softmax(tf.reshape(kpt_masks, [-1,d**2,17]),dim=1),[-1,d,d,17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Square tiling (rather than prime factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highestPrimeFactorization(n):    \n",
    "    return [(i, n//i) for i in range(1, int(n**0.5) + 1) if n % i == 0][-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFilterImage(filters):\n",
    "    \"\"\"\n",
    "    Takes as input a filter bank of size (1, H, W, C, D)\n",
    "    Returns: a tensor of size (1, sqrt(D)*H, sqrt(D)*H, C)\n",
    "    (This only really works for the first layer of filters with an image as input)\n",
    "    \"\"\"\n",
    "    padded_filters = tf.pad(filters,tf.constant([[0,0],[1,0],[1,0],[0,0],[0,0]]),'CONSTANT')\n",
    "    filter_list = tf.unstack(padded_filters,axis=4)\n",
    "    N = len(filter_list)\n",
    "    H = int(np.ceil(np.sqrt(N)))\n",
    "    W = int(np.floor(N/H))\n",
    "    diff = N - H*W\n",
    "    weight_strips = [tf.concat(filter_list[H*i:H*(i+1)],axis=1) for i in range(W)]\n",
    "    if diff > 0:\n",
    "        final_strip = tf.concat(filter_list[H*W:N],axis=1)\n",
    "        final_strip = tf.pad(final_strip,tf.constant([[0,0],[0,(H-diff)*padded_filters.shape.as_list()[1]],[0,0],[0,0]]),'CONSTANT')\n",
    "        weight_strips.append(final_strip)\n",
    "    weight_image = tf.concat(weight_strips,axis=2)\n",
    "    \n",
    "    return weight_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getActivationImage(activations, scale_up=False):\n",
    "    \"\"\"\n",
    "    Tiles an activation map into a square grayscale image\n",
    "    Takes as input an activation map of size (N, H, W, D)\n",
    "    Returns: a tensor of size (N, sqrt(D)*H, sqrt(D)*H, 1)\n",
    "    \"\"\"\n",
    "    padded_activations = tf.pad(activations,tf.constant([[0,0],[1,0],[1,0],[0,0]]),'CONSTANT')\n",
    "    expanded_activations = tf.expand_dims(padded_activations,axis=3)\n",
    "    activations_list = tf.unstack(expanded_activations,axis=4)\n",
    "    N = len(activations_list)\n",
    "    H = int(np.ceil(np.sqrt(N)))\n",
    "    W = int(np.floor(N/H))\n",
    "    diff = N - H*W\n",
    "    activation_strips = [tf.concat(activations_list[H*i:H*(i+1)],axis=1) for i in range(W)]\n",
    "    if diff > 0:\n",
    "        final_strip = tf.concat(activations_list[H*W:N],axis=1)\n",
    "        final_strip = tf.pad(final_strip,tf.constant([[0,0],[0,(H-diff)*padded_activations.shape.as_list()[1]],[0,0],[0,0]]),'CONSTANT')\n",
    "        activation_strips.append(final_strip)\n",
    "    activation_image = tf.concat(activation_strips,axis=2)            \n",
    "    if scale_up:\n",
    "        activation_image = tf.divide(activation_image, tf.reduce_max(activation_image))\n",
    "\n",
    "    return activation_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(train_init_op)\n",
    "activations = tf.layers.conv2d(images,64,(3,3),(1,1),padding='SAME')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "I, M, KM, P, L = sess.run([images, masks, kpt_masks, pts, labels])\n",
    "WI = getFilterImage(tf.expand_dims(tf.trainable_variables()[0],0))\n",
    "AI = getActivationImage(activations)\n",
    "plt.figure(figsize=[8,4])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(WI.eval()[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(AI.eval()[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Keypoint Visualizations Overlaid on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale = 2.0\n",
    "def keypointHeatMapOverlay(images, kpt_masks, d=d, scale=2.0, threshold=0.1, grayscale=True):  \n",
    "    images_scaled = tf.image.resize_bilinear(images, kpt_masks.shape[1:3]) # resize\n",
    "    image_tile = getFilterImage(tf.stack([images_scaled for i in range(17)],axis=4)) # tile\n",
    "    if grayscale == True:\n",
    "        image_tile = tf.reduce_mean(image_tile,axis=3,keep_dims=True) # grayscale\n",
    "    image_tile = tf.divide(image_tile, tf.reduce_max(image_tile)) # normalize\n",
    "\n",
    "    # normalize individual keypoint masks\n",
    "    keypoint_masks = tf.divide(kpt_masks, tf.reduce_max(kpt_masks,axis=[1,2], keep_dims=True))\n",
    "    keypoint_tile = getActivationImage(keypoint_masks) # tile\n",
    "#     keypoint_tile = tf.divide(keypoint_tile, tf.reduce_max(keypoint_tile)) # normalize\n",
    "    \n",
    "    image_tile = image_tile*tf.to_float(tf.less_equal(keypoint_tile,threshold)) # zero at keypoint locations?\n",
    "    keypoint_tile = tf.concat([keypoint_tile, tf.zeros_like(keypoint_tile),tf.zeros_like(keypoint_tile)],axis=3) # map to R color channel\n",
    "\n",
    "    flattened_tile = image_tile + scale * keypoint_tile\n",
    "    \n",
    "    return flattened_tile\n",
    "\n",
    "flattened_tile = keypointHeatMapOverlay(images, kpt_masks)\n",
    "\n",
    "I, KM, FL = sess.run([images, kpt_masks, flattened_tile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=[16,10])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(I[i,:,:,:]/255.0)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(FL[i]/scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kpts = tf.reshape(pts,[-1,2,17])\n",
    "indices = tf.to_int32(kpts)\n",
    "\n",
    "kp_mask1 = tf.one_hot(depth=d,indices=indices[:,1,:],axis=0)\n",
    "kp_mask2 = tf.one_hot(depth=d,indices=indices[:,0,:],axis=1)\n",
    "\n",
    "kp_masks = tf.matmul(tf.transpose(kp_mask1,(2,0,1)),tf.transpose(kp_mask2,(2,0,1)))\n",
    "kp_masks = tf.transpose(kp_masks,(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.stack([tf.linspace(0.0,d,d) for i in range(kpts.shape[-1])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pts.eval().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero labels for all images with padding (in case that's the problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cropped_image = tf.placeholder(tf.float32, [1,224,180,3])\n",
    "padded_image = tf.placeholder(tf.float32, [1,224,224,3])\n",
    "\n",
    "labels = tf.ones([1,17])\n",
    "square = tf.reduce_min(tf.to_float(tf.equal(cropped_image.shape, padded_image.shape)))\n",
    "\n",
    "labels = square * labels\n",
    "labels.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourglass rework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def HourGlassNet(graph, inputs=None, num_levels=5, base_filters = 64, scalar_summary_list=None, image_summary_list=None, histogram_summary_list=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - net \n",
    "    - summary lists\n",
    "    \"\"\"\n",
    "    ###################################################\n",
    "    ################### HOURGLASS 1 ###################\n",
    "    ###################################################\n",
    "    with graph.as_default():\n",
    "        # Initialize summaries\n",
    "        if scalar_summary_list is None:\n",
    "            scalar_summary_list = []\n",
    "        if image_summary_list is None:\n",
    "            image_summary_list = []\n",
    "        if histogram_summary_list is None:\n",
    "            histogram_summary_list = []\n",
    "            \n",
    "        bridges = {}\n",
    "        bases = {}\n",
    "        with tf.variable_scope('Hourglass'):\n",
    "            with tf.variable_scope('base'):\n",
    "                base = inputs\n",
    "\n",
    "            for level in range(num_levels):\n",
    "                # Bridge - maintain constant size\n",
    "                bridge_filters = base_filters * 2 ** level\n",
    "                with tf.variable_scope('level_{}_bridge'.format(level+1)):\n",
    "                    bridge = base\n",
    "                    for i in range(num_levels - level):\n",
    "                        bridge = tf.layers.dropout(bridge,rate=0.5)\n",
    "                        bridge = tf.layers.conv2d(bridge,bridge_filters,(3,3),strides=(1,1),padding='SAME')\n",
    "                        bridge = tf.layers.batch_normalization(bridge,axis=3)\n",
    "                        bridge = tf.nn.relu(bridge)\n",
    "                    bridges[level] = bridge\n",
    "\n",
    "                if level < num_levels - 1:\n",
    "                    with tf.variable_scope('level_{}'.format(level+1)):\n",
    "                        # Base - decrease size by factor of 2 for n\n",
    "                        base = tf.layers.dropout(base,rate=0.5)\n",
    "                        base = tf.layers.conv2d(base,bridge_filters,(3,3),strides=(2,2),padding='SAME')\n",
    "                        base = tf.layers.batch_normalization(base,axis=3)\n",
    "                        base = tf.nn.relu(base)\n",
    "                    \n",
    "            for i in range(num_levels):\n",
    "                print(i, bridges[i].shape)\n",
    "                    \n",
    "            for level in reversed(range(0,num_levels)):\n",
    "                # resize_bilinear or upconv?\n",
    "                # output = tf.image.resize_bilinear(ouput,size=2*output.shape[1:2])\n",
    "                with tf.variable_scope('level_{}_up'.format(level)):\n",
    "                    out_filters = int(base_filters * 2 ** (level-1))\n",
    "                    output = bridges[level]\n",
    "                    output = tf.layers.dropout(output,rate=0.5)\n",
    "                    output = tf.layers.conv2d_transpose(output,out_filters,(3,3),(2,2),padding='SAME')\n",
    "                    output = tf.layers.batch_normalization(output,axis=3) # HERE OR AFTER CONCAT???\n",
    "                    output = tf.nn.relu(output) # HERE OR AFTER CONCAT???\n",
    "\n",
    "                    if level > 0:\n",
    "                        bridges[level-1] = tf.concat([bridges[level-1],output],axis=3)\n",
    "                    \n",
    "            for i in reversed(range(num_levels)):\n",
    "                print(i, bridges[i].shape)\n",
    "                \n",
    "            return head[0], scalar_summary_list, image_summary_list, histogram_summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    images=tf.placeholder(tf.float32,[10,224,224,3])\n",
    "\n",
    "    print('Block1')\n",
    "    with tf.variable_scope('Block1'):\n",
    "        net,_,_,_ = HourGlassNet(graph, inputs=images, num_levels=5, base_filters = 64, scalar_summary_list=None, image_summary_list=None, histogram_summary_list=None)\n",
    "    \n",
    "    print('Block1')\n",
    "    with tf.variable_scope('Block2'):\n",
    "        net,_,_,_ = HourGlassNet(graph, inputs=images, num_levels=5, base_filters = 17, scalar_summary_list=None, image_summary_list=None, histogram_summary_list=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    images=tf.placeholder(tf.float32,[10,224,224,3])\n",
    "    num_levels=5\n",
    "    base_filters=64\n",
    "    inputs = tf.layers.conv2d(images,64,(3,3),(2,2),'SAME')\n",
    "    head = {}\n",
    "    \n",
    "    with tf.variable_scope('Hourglass'):\n",
    "        with tf.variable_scope('base'):\n",
    "            base = tf.layers.conv2d(inputs, base_filters, (3,3),strides=(1,1),padding='SAME')\n",
    "            base = tf.layers.batch_normalization(base,axis=3)\n",
    "            base = tf.nn.relu(base)\n",
    "\n",
    "        for level in range(num_levels):\n",
    "            # Bridge - maintain constant size\n",
    "            bridge_filters = base_filters * 2 ** level\n",
    "            with tf.variable_scope('level_{}_bridge'.format(level)):\n",
    "                bridge = base\n",
    "                for i in range(num_levels - level):\n",
    "                    bridge = tf.layers.dropout(bridge,rate=0.5)\n",
    "                    bridge = tf.layers.conv2d(bridge,bridge_filters,(3,3),strides=(1,1),padding='SAME')\n",
    "                    bridge = tf.layers.batch_normalization(bridge,axis=3)\n",
    "                    bridge = tf.nn.relu(bridge)\n",
    "                head[level] = bridge\n",
    "\n",
    "            with tf.variable_scope('level_{}'.format(level+1)):\n",
    "                # Base - decrease size by factor of 2 for n\n",
    "                base = tf.layers.dropout(base,rate=0.5)\n",
    "                base = tf.layers.conv2d(base,bridge_filters,(3,3),strides=(2,2),padding='SAME')\n",
    "                base = tf.layers.batch_normalization(base,axis=3)\n",
    "                base = tf.nn.relu(base)\n",
    "\n",
    "        for i in range(num_levels):\n",
    "            print(i, head[i].shape)\n",
    "            \n",
    "        for level in reversed(range(1,num_levels)):\n",
    "            # resize_bilinear or upconv?\n",
    "            # output = tf.image.resize_bilinear(ouput,size=2*output.shape[1:2])\n",
    "            with tf.variable_scope('level_{}_up'.format(level)):\n",
    "                out_filters = int(base_filters * 2 ** (level-1))\n",
    "                output = head[level]\n",
    "                output = tf.layers.dropout(output,rate=0.5)\n",
    "                output = tf.layers.conv2d_transpose(output,out_filters,(3,3),(2,2),padding='SAME')\n",
    "                output = tf.layers.batch_normalization(output,axis=3) # HERE OR AFTER CONCAT???\n",
    "                output = tf.nn.relu(output) # HERE OR AFTER CONCAT???\n",
    "\n",
    "                # if level > 0:\n",
    "                head[level-1] = tf.concat([head[level-1],output],axis=3)\n",
    "\n",
    "        for i in reversed(range(num_levels)):\n",
    "            print(i, head[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in head.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
