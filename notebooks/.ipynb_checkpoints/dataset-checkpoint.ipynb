{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "\n",
    "# import cv2\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kyle/Repositories/coco/annotations/person_keypoints_train2014.json\n",
      "/Users/kyle/Repositories/coco/annotations/person_keypoints_val2014.json\n",
      "/Users/kyle/Repositories/coco/annotations/image_info_test-dev2015.json\n",
      "/Users/kyle/Repositories/coco/images/person_keypoints_train2014\n",
      "/Users/kyle/Repositories/coco/images/person_keypoints_val2014\n",
      "/Users/kyle/Repositories/coco/images/image_info_test-dev2015\n",
      "loading annotations into memory...\n",
      "Done (t=8.57s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "dataDir='/Users/kyle/Repositories/coco'\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "annTrainFile='{}/annotations/{}.json'.format(dataDir,trainData)\n",
    "annValFile='{}/annotations/{}.json'.format(dataDir,valData)\n",
    "annTestFile='{}/annotations/{}.json'.format(dataDir,testData)\n",
    "\n",
    "imageTrainFolder='{}/images/{}'.format(dataDir,trainData)\n",
    "imageValFolder='{}/images/{}'.format(dataDir,valData)\n",
    "imageTestFolder='{}/images/{}'.format(dataDir,testData)\n",
    "\n",
    "print(annTrainFile)\n",
    "print(annValFile)\n",
    "print(annTestFile)\n",
    "print(imageTrainFolder)\n",
    "print(imageValFolder)\n",
    "print(imageTestFolder)\n",
    "\n",
    "# initialize a coco object\n",
    "coco = COCO(annTrainFile)\n",
    "\n",
    "# get all images containing the 'person' category\n",
    "catIds = coco.getCatIds(catNms=['person'])\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "catIds = imgIds[0:30]\n",
    "imgIds = imgIds[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize train_dataset\n",
    "\n",
    "baseDir = '/Users/kyle/Repositories/coco'\n",
    "imageTrainDir = 'train2014'\n",
    "annTrainDir = 'person_keypoints_val2014'\n",
    "imagePath = '{}/images/{}'.format(baseDir,imageTrainDir)\n",
    "annPath = '{}/annotations/{}'.format(baseDir,annTrainDir)\n",
    "\n",
    "\n",
    "filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(imagePath,img_ID) for img_ID in imgIds])\n",
    "imgID_tensor = tf.constant(imgIds)\n",
    "\n",
    "precision = tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_annotations(filename, imgID, coco=coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "    \n",
    "    return filename, bbox, keypoints, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def resize_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D = tf.constant(224.0)):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "    mask = tf.transpose([mask],[1,2,0])\n",
    "    bbox_tensor = tf.to_float(bbox_tensor)\n",
    "    keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "    sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "    centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "    centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "    center = tf.stack([centerX,centerY])\n",
    "\n",
    "    corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    i_shape = tf.subtract(corner2,corner1)\n",
    "    d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "    scale = tf.divide(D, sideLength)\n",
    "\n",
    "    cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "    cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "    dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "    dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "    pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "    pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "    pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "    pts = tf.multiply(pts,scale) # scale keypoints\n",
    "    \n",
    "    # set invalid pts to 0\n",
    "    inbounds = tf.less(pts,D)\n",
    "    inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "    pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "\n",
    "    padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "    padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "    resized_image = tf.image.resize_images(padded_image,tf.constant([224,224]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_mask = tf.image.resize_images(padded_mask,tf.constant([224,224]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return resized_image , resized_mask, pts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((224, 224, 3), (224, 224, ?), <unknown>, <unknown>), types: (tf.float32, tf.uint8, tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames,imgID_tensor))\n",
    "# Extract Annotations via coco interface\n",
    "train_dataset = train_dataset.map(lambda filename, imgID: tf.py_func(extract_annotations, [filename, imgID], \n",
    "                                                             [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "# All other preprocessing in tensorflow\n",
    "train_dataset = train_dataset.map(resize_image_tf)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((?, 224, 224, 3), (?, 224, 224, ?), <unknown>, <unknown>), types: (tf.float32, tf.uint8, tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BATCH\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "train_dataset = train_dataset.batch(10) # must resize images to make them match\n",
    "\n",
    "iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                                   train_dataset.output_shapes)\n",
    "# resized_image, resized_mask, pts, labels = iterator.get_next()\n",
    "images, masks, pts, labels = iterator.get_next()\n",
    "\n",
    "train_init_op = iterator.make_initializer(train_dataset)\n",
    "# val_init_op = iterator.make_initializer(val_dataset)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resized_image, resized_mask, pts, labels = sess.run(next_element)\n",
    "\n",
    "try:\n",
    "    I, M, P, L = sess.run([images, masks, pts, labels])\n",
    "    plt.imshow(I[0])\n",
    "    plt.imshow(M[0][:,:,0],alpha=0.5)\n",
    "    plt.scatter(P[0][(np.reshape(L[0],-1)==2),0],P[0][(np.reshape(L[0],-1)==2),1],c=\"r\")\n",
    "except tf.errors.OutOfRangeError:\n",
    "    sess.run(train_init_op)\n",
    "    print(\"Reinitialized Dataset Iterator...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "vgg = tf.contrib.slim.nets.vgg\n",
    "with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=.9)):\n",
    "    logits, _ = vgg.vgg_16(images, num_classes=50, is_training=is_training,dropout_keep_prob=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
