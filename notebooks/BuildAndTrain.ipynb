{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kyle/Repositories/coco/annotations/person_keypoints_train2014.json\n",
      "/Users/kyle/Repositories/coco/annotations/person_keypoints_val2014.json\n",
      "/Users/kyle/Repositories/coco/annotations/image_info_test_dev2015.json\n",
      "/Users/kyle/Repositories/coco/images/train2014\n",
      "/Users/kyle/Repositories/coco/images/val2014\n",
      "/Users/kyle/Repositories/coco/images/dev2015\n",
      "loading annotations into memory...\n",
      "Done (t=11.23s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainAnnDir='person_keypoints_train2014'\n",
    "valAnnDir='person_keypoints_val2014'\n",
    "testAnnDir='image_info_test_dev2015'\n",
    "\n",
    "trainDir='train2014'\n",
    "valDir='val2014'\n",
    "testDir='dev2015'\n",
    "\n",
    "trainAnnotations='{}/annotations/{}.json'.format(baseDir,trainAnnDir)\n",
    "valAnnotations='{}/annotations/{}.json'.format(baseDir,valAnnDir)\n",
    "testAnnotations='{}/annotations/{}.json'.format(baseDir,testAnnDir)\n",
    "\n",
    "trainData='{}/images/{}'.format(baseDir,trainDir)\n",
    "valData='{}/images/{}'.format(baseDir,valDir)\n",
    "testData='{}/images/{}'.format(baseDir,testDir)\n",
    "\n",
    "print(trainAnnotations)\n",
    "print(valAnnotations)\n",
    "print(testAnnotations)\n",
    "print(trainData)\n",
    "print(valData)\n",
    "print(testData)\n",
    "\n",
    "# initialize a coco object\n",
    "coco = COCO(trainAnnotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the appropriate images\n",
    "\n",
    "** Now that we have downloaded the labels and initialialized a COCO object, we can examine the organizational structure of the dataset and specify specific categories of images to download  ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all images containing the 'person' category\n",
    "catIds = coco.getCatIds(catNms=['person'])\n",
    "imgIds = coco.getImgIds(catIds=catIds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "batch_IDs = random.sample(imgIds, batch_size)\n",
    "filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(trainData,ID) for ID in batch_IDs])\n",
    "\n",
    "annotations = [coco.loadAnns(coco.getAnnIds(imgIds=ID, catIds=catIds, iscrowd=None)) for ID in batch_IDs] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill lists with batch \n",
    "\n",
    "precision = tf.float16\n",
    "\n",
    "bbox_list = [tf.stack([tf.constant(\n",
    "                ann['bbox'],dtype=precision\n",
    "            ) for ann in anns], axis=0) for anns in annotations]\n",
    "keypoints_list = [tf.stack([tf.constant(\n",
    "                np.reshape(ann['keypoints'],(-1,3)), dtype=precision\n",
    "            ) for ann in anns], axis=0) for anns in annotations]\n",
    "crowd_list = [tf.stack([tf.constant(\n",
    "                [ann['iscrowd']]\n",
    "            ) for ann in anns],axis=0) for anns in annotations]\n",
    "area_list = [tf.stack([tf.constant(\n",
    "                [ann['area']],dtype=precision\n",
    "            ) for ann in anns], axis=0) for anns in annotations]\n",
    "mask_list = [tf.stack([tf.constant(\n",
    "                coco.annToMask(ann), dtype=tf.bool\n",
    "            ) for ann in anns], axis = 0) for anns in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'stack_607:0' shape=(3, 305, 432) dtype=bool>,\n",
       " <tf.Tensor 'stack_608:0' shape=(13, 640, 427) dtype=bool>,\n",
       " <tf.Tensor 'stack_609:0' shape=(2, 427, 640) dtype=bool>,\n",
       " <tf.Tensor 'stack_610:0' shape=(1, 640, 427) dtype=bool>,\n",
       " <tf.Tensor 'stack_611:0' shape=(8, 640, 480) dtype=bool>,\n",
       " <tf.Tensor 'stack_612:0' shape=(2, 480, 640) dtype=bool>,\n",
       " <tf.Tensor 'stack_613:0' shape=(1, 640, 427) dtype=bool>,\n",
       " <tf.Tensor 'stack_614:0' shape=(1, 640, 480) dtype=bool>,\n",
       " <tf.Tensor 'stack_615:0' shape=(5, 360, 640) dtype=bool>,\n",
       " <tf.Tensor 'stack_616:0' shape=(13, 480, 640) dtype=bool>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bbox = tf.placeholder(tf.float32)\n",
    "keypoints = tf.placeholder(tf.float32)\n",
    "area = tf.placeholder(tf.float32)\n",
    "mask = tf.placeholder(tf.float32)\n",
    "iscrowd = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 120.48000336,  136.80999756,  161.25      ,  119.45999908], dtype=float32),\n",
       " array([ 255.,  154.,    2.,    0.,    0.,    0.,  251.,  151.,    2.,\n",
       "           0.,    0.,    0.,  243.,  152.,    2.,  241.,  164.,    2.,\n",
       "         238.,  164.,    2.,    0.,    0.,    0.,  242.,  174.,    2.,\n",
       "           0.,    0.,    0.,  264.,  170.,    2.,  201.,  207.,    2.,\n",
       "         200.,  205.,    2.,  173.,  221.,    2.,  172.,  218.,    2.,\n",
       "         133.,  240.,    2.,  133.,  236.,    2.], dtype=float32),\n",
       " array(3410.22119140625, dtype=float32),\n",
       " array([[ 243.11999512,  139.08000183,  254.02000427,  141.80000305,\n",
       "          257.20001221,  155.42999268,  251.75      ,  167.24000549,\n",
       "          247.66000366,  171.33000183,  263.10998535,  167.24000549,\n",
       "          281.73001099,  176.32000732,  270.38000488,  189.5       ,\n",
       "          264.47000122,  174.50999451,  241.75999451,  177.69000244,\n",
       "          211.33000183,  210.8500061 ,  189.52000427,  219.92999268,\n",
       "          166.80999756,  226.28999329,  148.63999939,  234.91999817,\n",
       "          131.83999634,  256.26998901,  120.48000336,  244.46000671,\n",
       "          122.75      ,  235.36999512,  148.19000244,  219.92999268,\n",
       "          175.44000244,  204.94000244,  176.80000305,  200.8500061 ,\n",
       "          185.88999939,  199.03999329,  210.86999512,  188.13000488,\n",
       "          235.8500061 ,  152.69999695,  239.02999878,  136.80999756,\n",
       "          243.58000183,  137.25999451]], dtype=float32),\n",
       " array(0.0, dtype=float32)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "ann = annotations[0][0]\n",
    "\n",
    "sess.run([bbox, keypoints, area, mask, iscrowd], \n",
    "         {bbox: ann['bbox'], keypoints: ann['keypoints'], area: ann['area'], mask: ann['segmentation'], iscrowd: ann['iscrowd']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': 3410.22125,\n",
       " 'bbox': [120.48, 136.81, 161.25, 119.46],\n",
       " 'category_id': 1,\n",
       " 'id': 421588,\n",
       " 'image_id': 261172,\n",
       " 'iscrowd': 0,\n",
       " 'keypoints': [255,\n",
       "  154,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  251,\n",
       "  151,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  243,\n",
       "  152,\n",
       "  2,\n",
       "  241,\n",
       "  164,\n",
       "  2,\n",
       "  238,\n",
       "  164,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  242,\n",
       "  174,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  264,\n",
       "  170,\n",
       "  2,\n",
       "  201,\n",
       "  207,\n",
       "  2,\n",
       "  200,\n",
       "  205,\n",
       "  2,\n",
       "  173,\n",
       "  221,\n",
       "  2,\n",
       "  172,\n",
       "  218,\n",
       "  2,\n",
       "  133,\n",
       "  240,\n",
       "  2,\n",
       "  133,\n",
       "  236,\n",
       "  2],\n",
       " 'num_keypoints': 13,\n",
       " 'segmentation': [[243.12,\n",
       "   139.08,\n",
       "   254.02,\n",
       "   141.8,\n",
       "   257.2,\n",
       "   155.43,\n",
       "   251.75,\n",
       "   167.24,\n",
       "   247.66,\n",
       "   171.33,\n",
       "   263.11,\n",
       "   167.24,\n",
       "   281.73,\n",
       "   176.32,\n",
       "   270.38,\n",
       "   189.5,\n",
       "   264.47,\n",
       "   174.51,\n",
       "   241.76,\n",
       "   177.69,\n",
       "   211.33,\n",
       "   210.85,\n",
       "   189.52,\n",
       "   219.93,\n",
       "   166.81,\n",
       "   226.29,\n",
       "   148.64,\n",
       "   234.92,\n",
       "   131.84,\n",
       "   256.27,\n",
       "   120.48,\n",
       "   244.46,\n",
       "   122.75,\n",
       "   235.37,\n",
       "   148.19,\n",
       "   219.93,\n",
       "   175.44,\n",
       "   204.94,\n",
       "   176.8,\n",
       "   200.85,\n",
       "   185.89,\n",
       "   199.04,\n",
       "   210.87,\n",
       "   188.13,\n",
       "   235.85,\n",
       "   152.7,\n",
       "   239.03,\n",
       "   136.81,\n",
       "   243.58,\n",
       "   137.26]]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
