{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "\n",
    "import resnet_v2 as resnet\n",
    "# import cv2\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keypoint_CrossEntropyLoss(prediction_maps, keypoint_masks, labels, L=5.0, scope=\"keypointLoss\"):\n",
    "    \"\"\"\n",
    "    heat_maps = predictions from network\n",
    "    keypoints (N,17,2) = actual keypoint locations\n",
    "    labels (N,17,1) = 0 if invalid, 1 if occluded, 2 if valid\n",
    "    \"\"\"\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=prediction_maps,labels=keypoint_masks)\n",
    "    labels = tf.reshape(labels,[-1,1,1,17])\n",
    "    losses = tf.multiply(losses,labels) # set loss to zero for invalid keypoints (labels=0)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keypoint_SquaredErrorLoss(prediction_maps, keypoint_masks, labels, L=5.0, scope=\"keypointLoss\"):\n",
    "    \"\"\"\n",
    "    heat_maps = predictions from network\n",
    "    keypoints (N,17,2) = actual keypoint locations\n",
    "    labels (N,17,1) = 0 if invalid, 1 if occluded, 2 if valid\n",
    "    \"\"\"\n",
    "    losses = tf.squared_difference(prediction_maps,keypoint_masks)\n",
    "    labels = tf.reshape(labels,[-1,1,1,17])\n",
    "    losses = tf.multiply(losses,labels) # set loss to zero for invalid keypoints (labels=0)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_data(base_dir,image_dir,ann_file):\n",
    "    image_path = '{}/images/{}'.format(baseDir,image_dir)\n",
    "    ann_path='{}/annotations/{}.json'.format(baseDir,ann_file)\n",
    "\n",
    "    return image_path, ann_path\n",
    "    \n",
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "imageTrainDir = 'train2014'\n",
    "imageValDir = 'val2014'\n",
    "imageTestDir = 'test2015'\n",
    "\n",
    "train_img_path, train_ann_path = get_data(baseDir,imageTrainDir,trainData)\n",
    "val_img_path, val_ann_path = get_data(baseDir,imageValDir,valData)\n",
    "\n",
    "# initialize a coco object\n",
    "coco = COCO(train_ann_path)\n",
    "\n",
    "# get all images containing the 'person' category\n",
    "catIds = coco.getCatIds(catNms=['person'])\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "catIds = imgIds[0:30]\n",
    "imgIds = imgIds[0:30]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    VGG_MEAN = tf.reshape(tf.constant([123.68, 116.78, 103.94]),[1,1,3])\n",
    "    NUM_KEYPOINTS = 17\n",
    "    BATCH_SIZE = 10\n",
    "    L = 10.0 # keypoint effective radius\n",
    "    \n",
    "    def extract_annotations(filename, imgID, coco=coco):\n",
    "        anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "        ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "        bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "        keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "        mask = coco.annToMask(ann)\n",
    "        \n",
    "        return filename, bbox, keypoints, mask\n",
    "    \n",
    "    def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D = tf.constant(256.0)):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        resized_image (N,D,D,3) - cropped, padded (if needed), scaled to square image of size D\n",
    "        resized_mask (N,D,D,1) - cropped, padded (if needed), scaled to square mask of size D\n",
    "        pts (N,2,17) - keypoint coordinates (i,j) scaled to match up with resized_image\n",
    "        labels (N,1,17) - values corresponding to pts: {0: invalid, 1:occluded, 2:valid}\n",
    "        \"\"\"\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "        mask = tf.transpose([mask],[1,2,0])\n",
    "        bbox_tensor = tf.to_float(bbox_tensor)\n",
    "        keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "        sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "        centerY = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "        centerX = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "        center = tf.stack([centerY,centerX])\n",
    "\n",
    "        corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                             tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                             tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "        i_shape = tf.subtract(corner2,corner1)\n",
    "        d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "        scale = tf.divide(D, sideLength)\n",
    "        cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                      tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "        cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                      tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "        dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "        dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "        pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "        pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "        pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "        pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "        # set invalid pts to 0\n",
    "        inbounds = tf.less(pts,D)\n",
    "        inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "        pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "        pts = tf.transpose(pts,[1,0])\n",
    "        labels = tf.transpose(labels,[1,0])\n",
    "\n",
    "        padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "        padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                    tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "        resized_image = tf.image.resize_images(padded_image,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        resized_image = resized_image - VGG_MEAN\n",
    "        resized_mask = tf.image.resize_images(padded_mask,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        return resized_image, resized_mask, pts, labels\n",
    "\n",
    "    def scaleDownMaskAndKeypoints(image, mask, pts, labels):\n",
    "        mask = tf.image.resize_images(mask,tf.constant([128,128]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        pts = tf.multiply(pts,tf.constant(0.5))\n",
    "        return image, mask, pts, labels\n",
    "    \n",
    "    def generate_keypoint_masks(image, mask, keypoints, labels, D=128.0, L=L):\n",
    "        X, Y = tf.meshgrid(tf.linspace(0.0,128.0,128),tf.linspace(0.0,128.0,128))\n",
    "        X = tf.reshape(X,[128,128,1])\n",
    "        Y = tf.reshape(Y,[128,128,1])\n",
    "        X_stack = tf.tile(X,tf.constant([1,1,17],dtype=tf.int32))\n",
    "        Y_stack = tf.tile(Y,tf.constant([1,1,17],dtype=tf.int32))\n",
    "\n",
    "        pts = tf.reshape(keypoints,[1,2,17])\n",
    "        ptsX, ptsY = tf.split(pts,[1,1],axis=1)\n",
    "        d1 = tf.square(tf.subtract(X_stack,ptsX))\n",
    "        d2 = tf.square(tf.subtract(Y_stack,ptsY))\n",
    "\n",
    "        pt_masks = tf.multiply(tf.divide(tf.constant(1.0),tf.add(d1,d2)+L),L)\n",
    "        return image, mask, pt_masks, pts, labels\n",
    "    \n",
    "    ########## DATASET ###########\n",
    "    \n",
    "    with tf.variable_scope(\"DataSet\"):\n",
    "        # Initialize train_dataset\n",
    "        filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path,imgID) for imgID in imgIds])\n",
    "        imgID_tensor = tf.constant(imgIds)\n",
    "\n",
    "        train_dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames,imgID_tensor))\n",
    "        # Extract Annotations via coco interface\n",
    "        train_dataset = train_dataset.map(lambda filename, imgID: tf.py_func(extract_annotations, [filename, imgID], \n",
    "                                                                     [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "        # All other preprocessing in tensorflow\n",
    "        train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "        train_dataset = train_dataset.map(scaleDownMaskAndKeypoints)\n",
    "        train_dataset = train_dataset.map(generate_keypoint_masks)\n",
    "\n",
    "        # BATCH\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "        train_dataset = train_dataset.batch(10) # must resize images to make them match\n",
    "        iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "        # resized_image, resized_mask, pts, labels = iterator.get_next()\n",
    "#         images, masks, pts, labels = iterator.get_next()\n",
    "        images, masks, kpt_masks, pts, labels = iterator.get_next()\n",
    "        train_init_op = iterator.make_initializer(train_dataset)\n",
    "        \n",
    "    \n",
    "    ##################################################################\n",
    "    ##################### BACKBONE ARCHITECTURE ######################\n",
    "    ##################################################################\n",
    "    \n",
    "    resnet_v2 = tf.contrib.slim.nets.resnet_v2\n",
    "    with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
    "        logits, endpoints = resnet_v2.resnet_v2_50(\n",
    "            inputs=images,\n",
    "            num_classes=10,\n",
    "            is_training=is_training,\n",
    "            reuse=None,\n",
    "            output_stride=16,\n",
    "            scope='resnet_v2_50'\n",
    "            )\n",
    "\n",
    "        model_path = 'checkpoints/resnet_v2_50.ckpt'\n",
    "        assert(os.path.isfile(model_path))\n",
    "        # Backbone Variables - remember to exclude all variables above backbone (including block4 and logits)\n",
    "        variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['resnet_v2_50/postnorm','resnet_v2_50/logits'])\n",
    "        # Head variables\n",
    "        # Note: We would need another set of variables and another initializer to capture the logits as well\n",
    "        other_variables = tf.contrib.framework.get_variables('resnet_v2_50/postnorm')\n",
    "\n",
    "    \n",
    "    ##################################################################\n",
    "    ####################### HEAD ARCHITECTURE ########################\n",
    "    ##################################################################\n",
    "    \n",
    "    HEAD_SCOPE = 'NetworkHead'\n",
    "    PREDICTION_THRESHOLD = 0.5\n",
    "\n",
    "    with tf.variable_scope(HEAD_SCOPE):\n",
    "        block1 = endpoints['resnet_v2_50/block1']\n",
    "        block2 = endpoints['resnet_v2_50/block2']\n",
    "        block3 = endpoints['resnet_v2_50/block3']\n",
    "        block4 = endpoints['resnet_v2_50/block4']\n",
    "\n",
    "        with tf.variable_scope('Layer1'):\n",
    "            b1 = tf.layers.conv2d(block1, 64, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "            b2 = tf.layers.conv2d(block2, 128, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "            b3 = tf.layers.conv2d(block3, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "            b4 = tf.layers.conv2d(block4, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope('Layer2'):\n",
    "            b1 = tf.layers.conv2d(block1, 32, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "\n",
    "            b2 = tf.layers.conv2d_transpose(b2, 32, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "            b3 = tf.layers.conv2d_transpose(b3, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "            b4 = tf.layers.conv2d_transpose(b4, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "\n",
    "            b2 = tf.slice(b2,[0,1,1,0],b2.shape - np.array([0, 2, 2, 0]))\n",
    "            b3 = tf.slice(b3,[0,1,1,0],b3.shape - np.array([0, 2, 2, 0]))\n",
    "            b4 = tf.slice(b4,[0,1,1,0],b4.shape - np.array([0, 2, 2, 0]))\n",
    "\n",
    "        with tf.variable_scope('BatchNorm'):\n",
    "            b1 = tf.layers.batch_normalization(b1)\n",
    "            b2 = tf.layers.batch_normalization(b2)\n",
    "            b3 = tf.layers.batch_normalization(b3)\n",
    "            b4 = tf.layers.batch_normalization(b4)\n",
    "\n",
    "        with tf.variable_scope('Funnel'):\n",
    "            head = tf.concat([b1,b2,b3,b4],axis=3)\n",
    "\n",
    "        with tf.variable_scope('MaskHead'):\n",
    "            mask_head = tf.layers.conv2d_transpose(head, 16, (3,3), (2,2), padding='VALID', activation=tf.nn.relu)\n",
    "            mask_head = tf.slice(mask_head,[0,1,1,0],mask_head.shape - np.array([0, 2, 2, 0]))\n",
    "            mask_head = tf.layers.conv2d(mask_head, 1, (1,1), (1,1), padding='SAME', activation=None)\n",
    "\n",
    "        with tf.variable_scope('KeypointHead'):\n",
    "            keypoint_head = tf.layers.conv2d_transpose(head, 32, (3,3), (2,2), padding='VALID', activation = tf.nn.relu)\n",
    "            keypoint_head = tf.slice(keypoint_head,[0,1,1,0],keypoint_head.shape - np.array([0, 2, 2, 0]))\n",
    "            keypoint_head = tf.layers.conv2d(keypoint_head, 17, (1,1), (1,1), padding='SAME', activation=None)\n",
    "\n",
    "        with tf.variable_scope('MaskPrediction'):\n",
    "            mask_prediction = tf.nn.sigmoid(mask_head)\n",
    "            mask_prediction = tf.to_float(tf.greater_equal(mask_prediction, PREDICTION_THRESHOLD))\n",
    "\n",
    "        with tf.variable_scope('KeypointsPrediction'):\n",
    "            keypoint_prediction = tf.nn.sigmoid(keypoint_head)\n",
    "            keypoint_prediction = tf.to_float(tf.greater_equal(keypoint_prediction, PREDICTION_THRESHOLD))\n",
    "\n",
    "            \n",
    "            \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        file_writer = tf.summary.FileWriter('/tmp/HourGlassNet/1')\n",
    "        file_writer.add_graph(sess.graph)\n",
    "        \n",
    "        # initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # initialize dataset\n",
    "        sess.run(train_init_op) \n",
    "        masks, kpt_masks, mask_pred, kpt_pred, mask_loss, kpt_loss = sess.run([masks, kpt_masks, maskPrediction, \n",
    "                                                             keypointPredictions, maskLoss, keypointLoss])\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(mask_pred[i][:,:,0])\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(kpt_pred[i][:,:,0])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(masks[i][:,:,0])\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(np.sum(kpt_masks[i],axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init_fn(sess)  # load the pretrained weights\n",
    "# sess.run(fc8_init)  # initialize the new fc8 layer\n",
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resized_image, resized_mask, pts, labels = sess.run(next_element)\n",
    "\n",
    "try:\n",
    "    I, M, P, L = sess.run([images, masks, pts, labels], {is_training: True})\n",
    "    plt.imshow(I[0])\n",
    "    plt.imshow(M[0][:,:,0],alpha=0.5)\n",
    "    plt.scatter(P[0][(np.reshape(L[0],-1)==2),0],P[0][(np.reshape(L[0],-1)==2),1],c=\"r\")\n",
    "except tf.errors.OutOfRangeError:\n",
    "    sess.run(train_init_op)\n",
    "    print(\"Reinitialized Dataset Iterator...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Playing around with ResNet V2 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "D = 225\n",
    "images = tf.placeholder(tf.float32, (BATCH_SIZE,D,D,3))\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "resnet_v2 = tf.contrib.slim.nets.resnet_v2\n",
    "with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
    "    logits, endpoints = resnet_v2.resnet_v2_50(\n",
    "        inputs=images,\n",
    "        num_classes=10,\n",
    "        is_training=is_training,\n",
    "        reuse=None,\n",
    "        output_stride=16,\n",
    "        scope='resnet_v2_50'\n",
    "        )\n",
    "\n",
    "# model_path = 'checkpoints/resnet_v2_50.ckpt'\n",
    "# assert(os.path.isfile(model_path))\n",
    "# # Backbone Variables - remember to exclude all variables above backbone (including block4 and logits)\n",
    "# variables_to_restore = tf.contrib.framework.get_variables_to_restore(exclude=['resnet_v2_50/block4','resnet_v2_50/postnorm','resnet_v2_50/logits'])\n",
    "# # Head variables\n",
    "# # Note: We would need another set of variables and another initializer to capture the logits as well\n",
    "# other_variables = tf.contrib.framework.get_variables('resnet_v2_50/block4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highestPrimeFactorization(n):    \n",
    "    return [(i, n//i) for i in range(1, int(n**0.5) + 1) if n % i == 0][-1]\n",
    "\n",
    "def getFilterImageSummary(filters,name=None):\n",
    "    padded_filters = tf.pad(filters,tf.constant([[0,0],[1,1],[1,1],[0,0],[0,0]]),'CONSTANT')\n",
    "    filter_list = tf.unstack(padded_filters,axis=4)    \n",
    "    H,W = highestPrimeFactorization(len(filter_list))\n",
    "    weight_strips = [tf.concat(filter_list[8*i:8*(i+1)],axis=1) for i in range(W)]\n",
    "    weight_image = tf.concat(weight_strips,axis=2)\n",
    "    return weight_image\n",
    "    \n",
    "def getActivationImageSummary(activations,name=None):\n",
    "    padded_activations = tf.pad(activations,tf.constant([[0,0],[1,0],[1,0],[0,0]]),'CONSTANT')\n",
    "    expanded_activations = tf.expand_dims(padded_activations,axis=3)\n",
    "    activations_list = tf.unstack(expanded_activations,axis=4)\n",
    "    H,W = highestPrimeFactorization(len(activations_list))\n",
    "    activation_strips = [tf.concat(activations_list[H*i:H*(i+1)],axis=1) for i in range(W)]\n",
    "    activation_image = tf.concat(activation_strips,axis=2)\n",
    "    return activation_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HEAD_SCOPE = 'NetworkHead'\n",
    "PREDICTION_THRESHOLD = 0.5\n",
    "\n",
    "with tf.variable_scope(HEAD_SCOPE):\n",
    "    block1 = endpoints['resnet_v2_50/block1']\n",
    "    block2 = endpoints['resnet_v2_50/block2']\n",
    "    block3 = endpoints['resnet_v2_50/block3']\n",
    "    block4 = endpoints['resnet_v2_50/block4']\n",
    "    \n",
    "    with tf.variable_scope('Layer1'):\n",
    "        b1 = tf.layers.conv2d(block1, 64, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "        b2 = tf.layers.conv2d(block2, 128, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "        b3 = tf.layers.conv2d(block3, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "        b4 = tf.layers.conv2d(block4, 128, kernel_size=(1,1), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "    \n",
    "    with tf.variable_scope('Layer2'):\n",
    "        b1 = tf.layers.conv2d(block1, 32, kernel_size=(3,3), strides=(1,1),padding='SAME',activation=tf.nn.relu)\n",
    "\n",
    "        b2 = tf.layers.conv2d_transpose(b2, 32, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "        b3 = tf.layers.conv2d_transpose(b3, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "        b4 = tf.layers.conv2d_transpose(b4, 64, kernel_size=(3,3), strides=(2,2),padding='VALID',activation=tf.nn.relu)\n",
    "\n",
    "        b2 = tf.slice(b2,[0,1,1,0],b2.shape - np.array([0, 2, 2, 0]))\n",
    "        b3 = tf.slice(b3,[0,1,1,0],b3.shape - np.array([0, 2, 2, 0]))\n",
    "        b4 = tf.slice(b4,[0,1,1,0],b4.shape - np.array([0, 2, 2, 0]))\n",
    "    \n",
    "    with tf.variable_scope('BatchNorm'):\n",
    "        b1 = tf.layers.batch_normalization(b1)\n",
    "        b2 = tf.layers.batch_normalization(b2)\n",
    "        b3 = tf.layers.batch_normalization(b3)\n",
    "        b4 = tf.layers.batch_normalization(b4)\n",
    "        \n",
    "    with tf.variable_scope('Funnel'):\n",
    "        head = tf.concat([b1,b2,b3,b4],axis=3)\n",
    "\n",
    "    with tf.variable_scope('MaskHead'):\n",
    "        mask_head = tf.layers.conv2d_transpose(head, 16, (3,3), (2,2), padding='VALID', activation=tf.nn.relu)\n",
    "        mask_head = tf.slice(mask_head,[0,1,1,0],mask_head.shape - np.array([0, 2, 2, 0]))\n",
    "        mask_head = tf.layers.conv2d(mask_head, 1, (1,1), (1,1), padding='SAME', activation=None)\n",
    "        \n",
    "    with tf.variable_scope('KeypointHead'):\n",
    "        keypoint_head = tf.layers.conv2d_transpose(head, 32, (3,3), (2,2), padding='VALID', activation = tf.nn.relu)\n",
    "        keypoint_head = tf.slice(keypoint_head,[0,1,1,0],keypoint_head.shape - np.array([0, 2, 2, 0]))\n",
    "        keypoint_head = tf.layers.conv2d(keypoint_head, 17, (1,1), (1,1), padding='SAME', activation=None)\n",
    "        \n",
    "    with tf.variable_scope('MaskPrediction'):\n",
    "        mask_prediction = tf.nn.sigmoid(mask_head)\n",
    "        mask_prediction = tf.to_float(tf.greater_equal(mask_prediction, PREDICTION_THRESHOLD))\n",
    "        \n",
    "    with tf.variable_scope('KeypointsPrediction'):\n",
    "        keypoint_prediction = tf.nn.sigmoid(keypoint_head)\n",
    "        keypoint_prediction = tf.to_float(tf.greater_equal(keypoint_prediction, PREDICTION_THRESHOLD))\n",
    "\n",
    "    \n",
    "mask_prediction.get_shape().as_list(), keypoint_prediction.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VGG_MEAN = tf.reshape(tf.constant([123.68, 116.78, 103.94]),[1,1,3])\n",
    "NUM_KEYPOINTS = 17\n",
    "BATCH_SIZE = 10\n",
    "L = 10.0 # keypoint effective radius\n",
    "\n",
    "def extract_annotations(filename, imgID, coco=coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "\n",
    "    return filename, bbox, keypoints, mask\n",
    "\n",
    "def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D = tf.constant(256.0)):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    resized_image (N,D,D,3) - cropped, padded (if needed), scaled to square image of size D\n",
    "    resized_mask (N,D,D,1) - cropped, padded (if needed), scaled to square mask of size D\n",
    "    pts (N,2,17) - keypoint coordinates (i,j) scaled to match up with resized_image\n",
    "    labels (N,1,17) - values corresponding to pts: {0: invalid, 1:occluded, 2:valid}\n",
    "    \"\"\"\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "    mask = tf.transpose([mask],[1,2,0])\n",
    "    bbox_tensor = tf.to_float(bbox_tensor)\n",
    "    keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "    sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "    centerY = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "    centerX = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "    center = tf.stack([centerY,centerX])\n",
    "\n",
    "    corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                         tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    i_shape = tf.subtract(corner2,corner1)\n",
    "    d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "    scale = tf.divide(D, sideLength)\n",
    "    cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "    cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                  tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "    dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "    dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "    pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "    pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "    pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "    pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "    # set invalid pts to 0\n",
    "    inbounds = tf.less(pts,D)\n",
    "    inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "    pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "    pts = tf.transpose(pts,[1,0])\n",
    "    labels = tf.transpose(labels,[1,0])\n",
    "\n",
    "    padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "    padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "    resized_image = tf.image.resize_images(padded_image,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    resized_image = resized_image - VGG_MEAN\n",
    "    resized_mask = tf.image.resize_images(padded_mask,tf.constant([256,256]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return resized_image, resized_mask, pts, labels\n",
    "\n",
    "def scaleDownMaskAndKeypoints(image, mask, pts, labels):\n",
    "    mask = tf.image.resize_images(mask,tf.constant([128,128]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    pts = tf.multiply(pts,tf.constant(0.5))\n",
    "    return image, mask, pts, labels\n",
    "\n",
    "def generate_keypoint_masks(image, mask, pts, labels, D=128.0, L=L):\n",
    "    X, Y = tf.meshgrid(tf.linspace(0.0,128.0,128),tf.linspace(0.0,128.0,128))\n",
    "    X = tf.reshape(X,[128,128,1])\n",
    "    Y = tf.reshape(Y,[128,128,1])\n",
    "    X_stack = tf.tile(X,tf.constant([1,1,17],dtype=tf.int32))\n",
    "    Y_stack = tf.tile(Y,tf.constant([1,1,17],dtype=tf.int32))\n",
    "\n",
    "    pts = tf.reshape(pts,[1,2,17])\n",
    "    ptsX, ptsY = tf.split(pts,[1,1],axis=1)\n",
    "    d1 = tf.square(tf.subtract(X_stack,ptsX))\n",
    "    d2 = tf.square(tf.subtract(Y_stack,ptsY))\n",
    "\n",
    "    pt_masks = tf.multiply(tf.divide(tf.constant(1.0),tf.add(d1,d2)+L),L)\n",
    "    return image, mask, pt_masks, pts, labels\n",
    "\n",
    "########## DATASET ###########\n",
    "\n",
    "with tf.variable_scope(\"DataSet\"):\n",
    "    # Initialize train_dataset\n",
    "    filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path,imgID) for imgID in imgIds])\n",
    "    imgID_tensor = tf.constant(imgIds)\n",
    "\n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((filenames,imgID_tensor))\n",
    "    # Extract Annotations via coco interface\n",
    "    train_dataset = train_dataset.map(lambda filename, imgID: tf.py_func(extract_annotations, [filename, imgID], \n",
    "                                                                 [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    # All other preprocessing in tensorflow\n",
    "    train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "    train_dataset = train_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    train_dataset = train_dataset.map(generate_keypoint_masks)\n",
    "\n",
    "    # BATCH\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.batch(10) # must resize images to make them match\n",
    "    iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "\n",
    "    images, masks, kpt_masks, pts, labels = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images.shape.as_list(), masks.shape.as_list(), kpt_masks.shape.as_list(), pts.shape.as_list(), labels.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KeypointPrediction(pred_masks, d):\n",
    "    \"\"\"\n",
    "    Input: Keypoint \"Heatmap\" Tensor\n",
    "    Output: Keypoint coordinates in tensor form\n",
    "    \"\"\"\n",
    "    x = tf.reshape(tf.linspace(0.5,d-0.5,d),[1,d,1,1])\n",
    "    pred = tf.multiply(kpt_masks, tf.to_float(tf.greater_equal(kpt_masks,0.5)))\n",
    "    pred_i = tf.reduce_sum(tf.multiply(pred, x),axis=[1,2])/tf.reduce_sum(pred,axis=[1,2])\n",
    "    pred_j = tf.reduce_sum(tf.multiply(pred, tf.transpose(x,(0,2,1,3))),axis=[1,2])/tf.reduce_sum(pred,axis=[1,2])\n",
    "    pred_pts = tf.stack([pred_j,pred_i],axis=1)\n",
    "    pred_pts = tf.expand_dims(pred_pts,axis=1)\n",
    "    return pred_pts\n",
    "\n",
    "def keypointPredictionAccuracy(pred_pts, true_pts, labels, threshold):\n",
    "    \"\"\"\n",
    "    Accuracy is a boolean: 1 if ||pred_pt-true_pt||^2 < threshold^2, 0 otherwise\n",
    "    \"\"\"\n",
    "    error = tf.multiply(tf.square(tf.subtract(pred_pts, true_pts)), tf.to_float(tf.greater_equal(labels, 1)))\n",
    "    accuracy = tf.reduce_mean(tf.to_float(tf.less(error,tf.square(threshold))))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ACC_THRESHOLD = 5.0\n",
    "pred_pts = KeypointPrediction(kpt_masks, 128)\n",
    "pred_mask = tf.random_uniform([10,128,128,1],0,2,tf.int32) # fake mask\n",
    "mask_accuracy = MaskAccuracy(pred_mask, masks)\n",
    "kp_accuracy = keypointPredictionAccuracy(pred_pts, pts, labels, ACC_THRESHOLD)\n",
    "I, M, KM, kPts, L, pPts, rP, kp_acc, m_acc = sess.run(\n",
    "    [images, masks, kpt_masks, pts, labels, pred_pts, ptsRev, kp_accuracy, mask_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,5])\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.sum(KM[0][:,:,:]*L[0],axis=2))\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(I[0])\n",
    "plt.subplot(1,3,3)\n",
    "plt.gca().set_facecolor('k')\n",
    "plt.scatter(kPts[0,0,0][L[0,0]!=0],-kPts[0,0,1][L[0,0]!=0],c='white',label=\"label\")\n",
    "plt.scatter(pPts[0,0,0][L[0,0]!=0],-pPts[0,0,1][L[0,0]!=0],c='red',s=15,label=\"prediction\")\n",
    "plt.axis(\"equal\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MaskAccuracy(pred_mask, true_mask):\n",
    "    overlap = tf.reduce_sum(tf.multiply(tf.to_float(pred_mask),tf.to_float(true_mask)),axis=[1,2,3])\n",
    "    score1 = tf.divide(overlap, tf.reduce_sum(tf.to_float(pred_mask),axis=[1,2,3]))\n",
    "    score2 = tf.divide(overlap, tf.reduce_sum(tf.to_float(true_mask),axis=[1,2,3]))\n",
    "    accuracy = tf.minimum(score1,score2)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHVCAYAAAAzabX0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIfn/ARrfFwEkUHUiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110e78048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,100,200)\n",
    "mu = 23\n",
    "L = 10.0\n",
    "y = L / (L + (x - L)**2)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
