{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "\n",
    "import resnet_v2 as resnet\n",
    "# import cv2\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing COCO objects to extract training and validation datasets...\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=7.79s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.11s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_data(base_dir,image_dir,ann_file):\n",
    "    image_path = '{}/images/{}'.format(baseDir,image_dir)\n",
    "    ann_path='{}/annotations/{}.json'.format(baseDir,ann_file)\n",
    "\n",
    "    return image_path, ann_path\n",
    "    \n",
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "imageTrainDir = 'train2014'\n",
    "imageValDir = 'val2014'\n",
    "imageTestDir = 'test2015'\n",
    "\n",
    "train_img_path, train_ann_path = get_data(baseDir,imageTrainDir,trainData)\n",
    "val_img_path, val_ann_path = get_data(baseDir,imageValDir,valData)\n",
    "# initialize a coco object\n",
    "print(\"Initializing COCO objects to extract training and validation datasets...\\n\")\n",
    "train_coco = COCO(train_ann_path)\n",
    "val_coco = COCO(val_ann_path)\n",
    "# get all images containing the 'person' category\n",
    "train_catIds = train_coco.getCatIds(catNms=['person'])\n",
    "train_imgIds = train_coco.getImgIds(catIds=train_catIds)\n",
    "val_catIds = val_coco.getCatIds(catNms=['person'])\n",
    "val_imgIds = val_coco.getImgIds(catIds=val_catIds)\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "# if args.small_dataset:\n",
    "train_catIds = train_catIds[0:30]\n",
    "train_imgIds = train_imgIds[0:30]\n",
    "val_catIds = val_catIds[0:30]\n",
    "val_imgIds = val_imgIds[0:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Dataset...\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "############### VARIOUS HYPER-PARAMETERS ##############\n",
    "#######################################################\n",
    "\n",
    "NUM_KEYPOINTS = 17\n",
    "BATCH_SIZE = 5 #args.batch_size\n",
    "L = 5.0 # keypoint effective radius\n",
    "D = 224 # image height and width\n",
    "d = 56 # evaluation height and width (for mask and keypoint masks)\n",
    "\n",
    "MASK_THRESHOLD = 0.5 # threshold for on/off prediction (in mask and keypoint masks)\n",
    "KP_THRESHOLD = 0.5 # threshold for on/off prediction (in mask and keypoint masks)\n",
    "KP_DISTANCE_THRESHOLD = 5.0 # threshold for determining if a keypoint estimate is accurate\n",
    "X_INIT = tf.contrib.layers.xavier_initializer_conv2d() # xavier initializer for head architecture\n",
    "# learning_rate1 = args.learning_rate1\n",
    "# learning_rate2 = args.learning_rate2\n",
    "\n",
    "#######################################################\n",
    "#### VISUALIZATION TOOLS - WEIGHTS AND ACTIVATIONS ####\n",
    "#######################################################\n",
    "def highestPrimeFactorization(n):    \n",
    "    return [(i, n//i) for i in range(1, int(n**0.5) + 1) if n % i == 0][-1] \n",
    "\n",
    "def getFilterImage(filters):\n",
    "    \"\"\"\n",
    "    Takes as input a filter bank of size (1, H, W, C, D)\n",
    "    Returns: a tensor of size (1, sqrt(D)*H, sqrt(D)*H, C)\n",
    "    (This only really works for the first layer of filtes with an image as input)\n",
    "    \"\"\"\n",
    "    padded_filters = tf.pad(filters,tf.constant([[0,0],[1,0],[1,0],[0,0],[0,0]]),'CONSTANT')\n",
    "    filter_list = tf.unstack(padded_filters,axis=4)\n",
    "    H,W = highestPrimeFactorization(len(filter_list))\n",
    "    weight_strips = [tf.concat(filter_list[8*i:8*(i+1)],axis=1) for i in range(W)]\n",
    "    weight_image = tf.concat(weight_strips,axis=2)\n",
    "    return weight_image\n",
    "\n",
    "def getActivationImage(activations):\n",
    "    \"\"\"\n",
    "    Tiles an activation map into a square grayscale image\n",
    "    Takes as input an activation map of size (N, H, W, D)\n",
    "    Returns: a tensor of size (N, sqrt(D)*H, sqrt(D)*H, 1)\n",
    "    \"\"\"\n",
    "    padded_activations = tf.pad(activations,tf.constant([[0,0],[1,0],[1,0],[0,0]]),'CONSTANT')\n",
    "    expanded_activations = tf.expand_dims(padded_activations,axis=3)\n",
    "    activations_list = tf.unstack(expanded_activations,axis=4)\n",
    "    H,W = highestPrimeFactorization(len(activations_list))\n",
    "    activation_strips = [tf.concat(activations_list[H*i:H*(i+1)],axis=1) for i in range(W)]\n",
    "    activation_image = tf.concat(activation_strips,axis=2)\n",
    "    return activation_image\n",
    "#######################################################\n",
    "##### PRE-PROCESSING AND DATASET EXTRACTION TOOLS #####\n",
    "#######################################################\n",
    "def extract_annotations_train(filename, imgID, coco=train_coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "    return filename, bbox, keypoints, mask\n",
    "\n",
    "def extract_annotations_val(filename, imgID, coco=val_coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "    return filename, bbox, keypoints, mask\n",
    "\n",
    "def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D=D):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    resized_image (N,D,D,3) - cropped, padded (if needed), scaled to square image of size D\n",
    "    resized_mask (N,D,D,1) - cropped, padded (if needed), scaled to square mask of size D\n",
    "    pts (N,2,17) - keypoint coordinates (i,j) scaled to match up with resized_image\n",
    "    labels (N,1,17) - values corresponding to pts: {0: invalid, 1:occluded, 2:valid}\n",
    "    \"\"\"\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "    mask = tf.transpose([mask],[1,2,0])\n",
    "    bbox_tensor = tf.to_float(bbox_tensor)\n",
    "    keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "    sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "    centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "    centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "    center = tf.stack([centerX,centerY])\n",
    "\n",
    "    corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                        tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                        tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    i_shape = tf.subtract(corner2,corner1)\n",
    "    d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "    scale = tf.divide(tf.constant(D,tf.float32), sideLength)\n",
    "    cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "    cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "    dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "    dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "    pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "    pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "    pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "    pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "    # set invalid pts to 0\n",
    "    inbounds = tf.less(pts,tf.constant(D,tf.float32))\n",
    "    inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "    pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "    pts = tf.transpose(pts,[1,0])\n",
    "    labels = tf.transpose(labels,[1,0])\n",
    "\n",
    "    padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "    padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "    resized_image = tf.image.resize_images(padded_image,tf.constant([D,D]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # resized_image = resized_image - VGG_MEAN\n",
    "    resized_mask = tf.image.resize_images(padded_mask,tf.constant([D,D]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return resized_image, resized_mask, pts, labels\n",
    "\n",
    "def scaleDownMaskAndKeypoints(image, mask, pts, labels, d=d, D=D):\n",
    "    mask = tf.image.resize_images(mask,tf.constant([d,d]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    pts = tf.multiply(pts,tf.constant(d/D))\n",
    "    return image, mask, pts, labels\n",
    "\n",
    "def generate_keypoint_masks(image, mask, keypoints, labels, d=d, D=D, L=L):\n",
    "    X, Y = tf.meshgrid(tf.linspace(0.0,d,d),tf.linspace(0.0,d,d))\n",
    "    X = tf.reshape(X,[d,d,1])\n",
    "    Y = tf.reshape(Y,[d,d,1])\n",
    "    X_stack = tf.tile(X,tf.constant([1,1,17],dtype=tf.int32))\n",
    "    Y_stack = tf.tile(Y,tf.constant([1,1,17],dtype=tf.int32))\n",
    "\n",
    "    pts = tf.reshape(keypoints,[1,2,17])\n",
    "    ptsX, ptsY = tf.split(pts,[1,1],axis=1)\n",
    "    d1 = tf.square(tf.subtract(X_stack,ptsX))\n",
    "    d2 = tf.square(tf.subtract(Y_stack,ptsY))\n",
    "\n",
    "    pt_masks = tf.multiply(tf.divide(tf.constant(1.0),tf.add(d1,d2)+L),L)\n",
    "\n",
    "    return image, mask, pt_masks, pts, labels\n",
    "\n",
    "def generate_one_hot_keypoint_masks(image, mask, keypoints, labels, d=d):\n",
    "    pts = tf.reshape(keypoints,[1,2,17])\n",
    "    indices = tf.to_int32(pts)\n",
    "    kp_mask1 = tf.one_hot(depth=d,indices=indices[:,1,:],axis=0)\n",
    "    kp_mask2 = tf.one_hot(depth=d,indices=indices[:,0,:],axis=1)\n",
    "    kp_masks = tf.matmul(tf.transpose(kp_mask1,(2,0,1)),tf.transpose(kp_mask2,(2,0,1)))\n",
    "    kp_masks = tf.transpose(kp_masks,(1,2,0))\n",
    "    return image, mask, kp_masks, pts, labels\n",
    "\n",
    "\n",
    "#######################################################\n",
    "################## SUMMARY DICTIONARY #################\n",
    "#######################################################\n",
    "\n",
    "image_summary_list = []\n",
    "scalar_summary_list = []\n",
    "\n",
    "#######################################################\n",
    "################### PREPARE DATASET ###################\n",
    "#######################################################\n",
    "print(\"Initializing Dataset...\\n\")\n",
    "with tf.variable_scope('DataSet'):\n",
    "    ################### TRAIN DATASET ###################\n",
    "    train_filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path, imgID) for imgID in train_imgIds])\n",
    "    train_imgID_tensor = tf.constant(train_imgIds)\n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((train_filenames, train_imgID_tensor))\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda filename, imgID: tf.py_func(extract_annotations_train, [filename, imgID], [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "    train_dataset = train_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    train_dataset = train_dataset.map(generate_one_hot_keypoint_masks)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    #################### VAL DATASET ####################\n",
    "    val_filenames = tf.constant(['{}/COCO_val2014_{:0>12}.jpg'.format(val_img_path, imgID) for imgID in val_imgIds])\n",
    "    val_imgID_tensor = tf.constant(val_imgIds)\n",
    "    val_dataset = tf.contrib.data.Dataset.from_tensor_slices((val_filenames, val_imgID_tensor))\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda filename, imgID: tf.py_func(extract_annotations_val,[filename, imgID],[filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    val_dataset = val_dataset.map(preprocess_image_tf)\n",
    "    val_dataset = val_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    val_dataset = val_dataset.map(generate_one_hot_keypoint_masks)\n",
    "    val_dataset = val_dataset.shuffle(buffer_size=10000)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "\n",
    "    images, masks, kpt_masks, pts, labels = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    val_init_op = iterator.make_initializer(val_dataset)\n",
    "\n",
    "    image_summary_list.append(tf.summary.image('keypoint_masks', getActivationImage(kpt_masks)))\n",
    "    image_summary_list.append(tf.summary.image('input_images', images))\n",
    "\n",
    "#######################################################\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_keypoint_masks(kpt_masks, d=56):\n",
    "    return tf.reshape(tf.nn.softmax(tf.reshape(kpt_masks, [-1,d**2,17]),dim=1),[-1,d,d,17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNetMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "resnet_v2 = tf.contrib.slim.nets.resnet_v2\n",
    "with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n",
    "    logits, endpoints = resnet_v2.resnet_v2_50(\n",
    "        inputs=images, num_classes=2, is_training=is_training, reuse=None,\n",
    "        output_stride=16, scope='resnet_v2_50')\n",
    "\n",
    "# Model Path to ResNet v2 50 checkpoint\n",
    "model_path = '/Users/kyle/Documents/SCHOOL/Stanford/STANFORD_2016_to_2017/3rd_Quarter/CS231N/Project/checkpoints/resnet_v2_50.ckpt'\n",
    "assert(os.path.isfile(model_path))\n",
    "# Backbone Variables - remember to exclude all variables above backbone (including block4 and logits)\n",
    "backbone_variables = tf.contrib.framework.get_variables_to_restore(exclude=['resnet_v2_50/postnorm','resnet_v2_50/logits'])\n",
    "init_fn = tf.contrib.framework.assign_from_checkpoint_fn(model_path, backbone_variables) # Call to load pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_v2_50/conv1\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv1\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv2\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2/conv3\n",
      "resnet_v2_50/block1/unit_1/bottleneck_v2\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv1\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv2\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2/conv3\n",
      "resnet_v2_50/block1/unit_2/bottleneck_v2\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv1\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv2\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2/conv3\n",
      "resnet_v2_50/block1/unit_3/bottleneck_v2\n",
      "resnet_v2_50/block1\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv1\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv2\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2/conv3\n",
      "resnet_v2_50/block2/unit_1/bottleneck_v2\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv1\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv2\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2/conv3\n",
      "resnet_v2_50/block2/unit_2/bottleneck_v2\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv1\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv2\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2/conv3\n",
      "resnet_v2_50/block2/unit_3/bottleneck_v2\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv1\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv2\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2/conv3\n",
      "resnet_v2_50/block2/unit_4/bottleneck_v2\n",
      "resnet_v2_50/block2\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv1\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv2\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2/conv3\n",
      "resnet_v2_50/block3/unit_1/bottleneck_v2\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv1\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv2\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2/conv3\n",
      "resnet_v2_50/block3/unit_2/bottleneck_v2\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv1\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv2\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2/conv3\n",
      "resnet_v2_50/block3/unit_3/bottleneck_v2\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv1\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv2\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2/conv3\n",
      "resnet_v2_50/block3/unit_4/bottleneck_v2\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv1\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv2\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2/conv3\n",
      "resnet_v2_50/block3/unit_5/bottleneck_v2\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv1\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv2\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2/conv3\n",
      "resnet_v2_50/block3/unit_6/bottleneck_v2\n",
      "resnet_v2_50/block3\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv1\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv2\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2/conv3\n",
      "resnet_v2_50/block4/unit_1/bottleneck_v2\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv1\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv2\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2/conv3\n",
      "resnet_v2_50/block4/unit_2/bottleneck_v2\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv1\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv2\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2/conv3\n",
      "resnet_v2_50/block4/unit_3/bottleneck_v2\n",
      "resnet_v2_50/block4\n",
      "resnet_v2_50/logits\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "for k in endpoints.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bilinear_filter(channels_in,channels_out):\n",
    "    f = tf.multiply(tf.constant([0.5, 1.0, 0.5],shape=[3,1]),tf.constant([0.5, 1.0, 0.5],shape=[1,3]))\n",
    "    f = tf.stack([f for i in range(channels_out)],axis=2)\n",
    "    f = tf.stack([f for i in range(channels_in)],axis=3)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block1 = endpoints['resnet_v2_50/block1']\n",
    "block2 = endpoints['resnet_v2_50/block2']\n",
    "block3 = endpoints['resnet_v2_50/block3']\n",
    "block4 = endpoints['resnet_v2_50/block4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([None, 28, 28, 256],\n",
       " [None, 14, 14, 512],\n",
       " [None, 14, 14, 1024],\n",
       " [None, 14, 14, 2048])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block1.shape.as_list(),block2.shape.as_list(),block3.shape.as_list(),block4.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('block3_head'):\n",
    "    # initialize as bilinear upsampling\n",
    "    input_shape=block3.shape.as_list()\n",
    "    w3 = bilinear_filter(input_shape[3],128)\n",
    "    b3 = tf.nn.conv2d_transpose(block3,w3,output_shape=tf.constant(\n",
    "            [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "    b3 = tf.layers.batch_normalization(b3,axis=3) # axis=3 --> IMPORTANT!!\n",
    "    b3 = tf.nn.relu(b3)\n",
    "with tf.variable_scope('block4_head'):\n",
    "    input_shape=block4.shape.as_list()\n",
    "    w4 = bilinear_filter(input_shape[3],128)\n",
    "    b4 = tf.nn.conv2d_transpose(block4,w4,output_shape=tf.constant(\n",
    "            [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "    b4 = tf.layers.batch_normalization(b4,axis=3) # axis=3 --> IMPORTANT!!\n",
    "    b4 = tf.nn.relu(b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 28, 28, 128], [1, 28, 28, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b3.shape.as_list(), b4.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.concat([b3,b4],axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28, 28, 256]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape=net.shape.as_list()\n",
    "w = bilinear_filter(input_shape[3],128)\n",
    "net = tf.nn.conv2d_transpose(net,w,output_shape=tf.constant(\n",
    "        [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "net = tf.layers.batch_normalization(net,axis=3) # axis=3 --> IMPORTANT!!\n",
    "net = tf.nn.relu(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 56, 56, 128]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.conv2d(net,17,(1,1),(1,1),kernel_initializer=X_INIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 56, 56, 17]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('keypoint_predictions'):\n",
    "    keypoint_predictions = softmax_keypoint_masks(logits, d=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 56, 56, 17]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoint_predictions.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block1 = endpoints['resnet_v2_50/block1']\n",
    "block2 = endpoints['resnet_v2_50/block2']\n",
    "block3 = endpoints['resnet_v2_50/block3']\n",
    "block4 = endpoints['resnet_v2_50/block4']\n",
    "\n",
    "with tf.variable_scope('block3_head'):\n",
    "    # initialize as bilinear upsampling\n",
    "    input_shape=block3.shape.as_list()\n",
    "    w3 = bilinear_filter(input_shape[3],128)\n",
    "    b3 = tf.nn.conv2d_transpose(block3,w3,output_shape=tf.constant(\n",
    "            [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "    b3 = tf.layers.batch_normalization(b3,axis=3) # axis=3 --> IMPORTANT!!\n",
    "    b3 = tf.nn.relu(b3)\n",
    "with tf.variable_scope('block4_head'):\n",
    "    input_shape=block4.shape.as_list()\n",
    "    w4 = bilinear_filter(input_shape[3],128)\n",
    "    b4 = tf.nn.conv2d_transpose(block4,w4,output_shape=tf.constant(\n",
    "            [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "    b4 = tf.layers.batch_normalization(b4,axis=3) # axis=3 --> IMPORTANT!!\n",
    "    b4 = tf.nn.relu(b4)\n",
    "    \n",
    "net = tf.concat([b3,b4],axis=3)\n",
    "input_shape=net.shape.as_list()\n",
    "w = bilinear_filter(input_shape[3],128)\n",
    "net = tf.nn.conv2d_transpose(net,w,output_shape=tf.constant(\n",
    "        [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "net = tf.layers.batch_normalization(net,axis=3) # axis=3 --> IMPORTANT!!\n",
    "net = tf.nn.relu(net)\n",
    "\n",
    "logits = tf.layers.conv2d(net,17,(1,1),(1,1),kernel_initializer=X_INIT)\n",
    "\n",
    "with tf.variable_scope('keypoint_predictions'):\n",
    "    keypoint_predictions = softmax_keypoint_masks(logits, d=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Keypoint One-hot masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(train_init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = bilinear_filter(128,24)\n",
    "f.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = tf.placeholder(tf.float32,[1,20,20,128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = tf.layers.conv2d_transpose(image,strides=(2,2),kernel_initializer=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.conv2d_transpose(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SoftmaxKeypointMask(kpt_masks, d=56):\n",
    "    return tf.reshape(tf.nn.softmax(tf.reshape(kpt_masks, [-1,d**2,17]),dim=1),[-1,d,d,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SoftmaxKeypointMask(kpt_masks, d=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(bilinear_filter(128,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.nn.conv2d_transpose(image,w,tf.constant([1,40,40,24]),strides=(2,2),padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53312/(56**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir('/tmp/KyleNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
