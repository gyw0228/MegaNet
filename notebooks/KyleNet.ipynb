{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "\n",
    "import resnet_v2 as resnet\n",
    "# import cv2\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(base_dir,image_dir,ann_file):\n",
    "    image_path = '{}/images/{}'.format(baseDir,image_dir)\n",
    "    ann_path='{}/annotations/{}.json'.format(baseDir,ann_file)\n",
    "\n",
    "    return image_path, ann_path\n",
    "    \n",
    "# define the path to the annotation file corresponding to the images you want to work with\n",
    "baseDir='/Users/kyle/Repositories/coco'\n",
    "\n",
    "trainData='person_keypoints_train2014'\n",
    "valData='person_keypoints_val2014'\n",
    "testData='image_info_test-dev2015'\n",
    "\n",
    "imageTrainDir = 'train2014'\n",
    "imageValDir = 'val2014'\n",
    "imageTestDir = 'test2015'\n",
    "\n",
    "train_img_path, train_ann_path = get_data(baseDir,imageTrainDir,trainData)\n",
    "val_img_path, val_ann_path = get_data(baseDir,imageValDir,valData)\n",
    "# initialize a coco object\n",
    "print(\"Initializing COCO objects to extract training and validation datasets...\\n\")\n",
    "train_coco = COCO(train_ann_path)\n",
    "val_coco = COCO(val_ann_path)\n",
    "# get all images containing the 'person' category\n",
    "train_catIds = train_coco.getCatIds(catNms=['person'])\n",
    "train_imgIds = train_coco.getImgIds(catIds=train_catIds)\n",
    "val_catIds = val_coco.getCatIds(catNms=['person'])\n",
    "val_imgIds = val_coco.getImgIds(catIds=val_catIds)\n",
    "# Just for dealing with the images on my computer (not necessary when working with the whole dataset)\n",
    "# if args.small_dataset:\n",
    "train_catIds = train_catIds[0:30]\n",
    "train_imgIds = train_imgIds[0:30]\n",
    "val_catIds = val_catIds[0:30]\n",
    "val_imgIds = val_imgIds[0:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "############### VARIOUS HYPER-PARAMETERS ##############\n",
    "#######################################################\n",
    "\n",
    "NUM_KEYPOINTS = 17\n",
    "BATCH_SIZE = 5 #args.batch_size\n",
    "L = 5.0 # keypoint effective radius\n",
    "D = 224 # image height and width\n",
    "d = 56 # evaluation height and width (for mask and keypoint masks)\n",
    "\n",
    "MASK_THRESHOLD = 0.5 # threshold for on/off prediction (in mask and keypoint masks)\n",
    "KP_THRESHOLD = 0.5 # threshold for on/off prediction (in mask and keypoint masks)\n",
    "KP_DISTANCE_THRESHOLD = 5.0 # threshold for determining if a keypoint estimate is accurate\n",
    "X_INIT = tf.contrib.layers.xavier_initializer_conv2d() # xavier initializer for head architecture\n",
    "# learning_rate1 = args.learning_rate1\n",
    "# learning_rate2 = args.learning_rate2\n",
    "\n",
    "#######################################################\n",
    "#### VISUALIZATION TOOLS - WEIGHTS AND ACTIVATIONS ####\n",
    "#######################################################\n",
    "def highestPrimeFactorization(n):    \n",
    "    return [(i, n//i) for i in range(1, int(n**0.5) + 1) if n % i == 0][-1] \n",
    "\n",
    "def getFilterImage(filters):\n",
    "    \"\"\"\n",
    "    Takes as input a filter bank of size (1, H, W, C, D)\n",
    "    Returns: a tensor of size (1, sqrt(D)*H, sqrt(D)*H, C)\n",
    "    (This only really works for the first layer of filtes with an image as input)\n",
    "    \"\"\"\n",
    "    padded_filters = tf.pad(filters,tf.constant([[0,0],[1,0],[1,0],[0,0],[0,0]]),'CONSTANT')\n",
    "    filter_list = tf.unstack(padded_filters,axis=4)\n",
    "    H,W = highestPrimeFactorization(len(filter_list))\n",
    "    weight_strips = [tf.concat(filter_list[8*i:8*(i+1)],axis=1) for i in range(W)]\n",
    "    weight_image = tf.concat(weight_strips,axis=2)\n",
    "    return weight_image\n",
    "\n",
    "def getActivationImage(activations):\n",
    "    \"\"\"\n",
    "    Tiles an activation map into a square grayscale image\n",
    "    Takes as input an activation map of size (N, H, W, D)\n",
    "    Returns: a tensor of size (N, sqrt(D)*H, sqrt(D)*H, 1)\n",
    "    \"\"\"\n",
    "    padded_activations = tf.pad(activations,tf.constant([[0,0],[1,0],[1,0],[0,0]]),'CONSTANT')\n",
    "    expanded_activations = tf.expand_dims(padded_activations,axis=3)\n",
    "    activations_list = tf.unstack(expanded_activations,axis=4)\n",
    "    H,W = highestPrimeFactorization(len(activations_list))\n",
    "    activation_strips = [tf.concat(activations_list[H*i:H*(i+1)],axis=1) for i in range(W)]\n",
    "    activation_image = tf.concat(activation_strips,axis=2)\n",
    "    return activation_image\n",
    "#######################################################\n",
    "##### PRE-PROCESSING AND DATASET EXTRACTION TOOLS #####\n",
    "#######################################################\n",
    "def extract_annotations_train(filename, imgID, coco=train_coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "    return filename, bbox, keypoints, mask\n",
    "\n",
    "def extract_annotations_val(filename, imgID, coco=val_coco):\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgID,catIds=[1],iscrowd=None))\n",
    "    ann = max([ann for ann in anns], key=lambda item:item['area']) # extract annotation for biggest instance\n",
    "    bbox = np.array(np.floor(ann['bbox']),dtype=int)\n",
    "    keypoints = np.reshape(ann['keypoints'],(-1,3))\n",
    "    mask = coco.annToMask(ann)\n",
    "    return filename, bbox, keypoints, mask\n",
    "\n",
    "def preprocess_image_tf(filename, bbox_tensor, keypoints_tensor, mask, D=D):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    resized_image (N,D,D,3) - cropped, padded (if needed), scaled to square image of size D\n",
    "    resized_mask (N,D,D,1) - cropped, padded (if needed), scaled to square mask of size D\n",
    "    pts (N,2,17) - keypoint coordinates (i,j) scaled to match up with resized_image\n",
    "    labels (N,1,17) - values corresponding to pts: {0: invalid, 1:occluded, 2:valid}\n",
    "    \"\"\"\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "\n",
    "    mask = tf.transpose([mask],[1,2,0])\n",
    "    bbox_tensor = tf.to_float(bbox_tensor)\n",
    "    keypoints_tensor = tf.to_float(keypoints_tensor)\n",
    "\n",
    "    sideLength = tf.reduce_max(bbox_tensor[2:],axis=0)\n",
    "    centerX = tf.floor(bbox_tensor[0] + tf.divide(bbox_tensor[2],tf.constant(2.0)))\n",
    "    centerY = tf.floor(bbox_tensor[1] + tf.divide(bbox_tensor[3],tf.constant(2.0)))\n",
    "    center = tf.stack([centerX,centerY])\n",
    "\n",
    "    corner1 = tf.to_int32(tf.minimum(tf.maximum(tf.subtract(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                        tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    corner2 = tf.to_int32(tf.minimum(tf.maximum(tf.add(center, tf.divide(sideLength,tf.constant(2.0))),0),\n",
    "                        tf.reverse(tf.to_float(tf.shape(image)[:2]),tf.constant([0]))))\n",
    "    i_shape = tf.subtract(corner2,corner1)\n",
    "    d_shape = tf.subtract(tf.to_int32(sideLength),i_shape)\n",
    "\n",
    "    scale = tf.divide(tf.constant(D,tf.float32), sideLength)\n",
    "    cropped_image = tf.image.crop_to_bounding_box(image,corner1[1],corner1[0],\n",
    "                                                tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "    cropped_mask = tf.image.crop_to_bounding_box(mask,corner1[1],corner1[0],\n",
    "                                                tf.subtract(corner2,corner1)[1],tf.subtract(corner2,corner1)[0])\n",
    "\n",
    "    dX = tf.floor(tf.divide(d_shape,tf.constant(2)))\n",
    "    dY = tf.ceil(tf.divide(d_shape,tf.constant(2)))\n",
    "\n",
    "    pts, labels = tf.split(keypoints_tensor,[2,1],axis=1)\n",
    "    pts = tf.subtract(pts,tf.to_float(corner1)) # shift keypoints\n",
    "    pts = tf.add(pts,tf.to_float(dX)) # shift keypoints\n",
    "    pts = tf.multiply(pts,scale) # scale keypoints\n",
    "\n",
    "    # set invalid pts to 0\n",
    "    inbounds = tf.less(pts,tf.constant(D,tf.float32))\n",
    "    inbounds = tf.multiply(tf.to_int32(inbounds), tf.to_int32(tf.greater(pts,0)))\n",
    "    pts = tf.multiply(pts,tf.to_float(inbounds))\n",
    "    pts = tf.transpose(pts,[1,0])\n",
    "    labels = tf.transpose(labels,[1,0])\n",
    "\n",
    "    padded_image = tf.image.pad_to_bounding_box(cropped_image,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "    padded_mask = tf.image.pad_to_bounding_box(cropped_mask,tf.to_int32(dX[1]),tf.to_int32(dX[0]),\n",
    "                                                tf.to_int32(sideLength),tf.to_int32(sideLength))\n",
    "\n",
    "    resized_image = tf.image.resize_images(padded_image,tf.constant([D,D]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # resized_image = resized_image - VGG_MEAN\n",
    "    resized_mask = tf.image.resize_images(padded_mask,tf.constant([D,D]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return resized_image, resized_mask, pts, labels\n",
    "\n",
    "def scaleDownMaskAndKeypoints(image, mask, pts, labels, d=d, D=D):\n",
    "    mask = tf.image.resize_images(mask,tf.constant([d,d]),tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    pts = tf.multiply(pts,tf.constant(d/D))\n",
    "    return image, mask, pts, labels\n",
    "\n",
    "def generate_keypoint_masks(image, mask, keypoints, labels, d=d, D=D, L=L):\n",
    "    X, Y = tf.meshgrid(tf.linspace(0.0,d,d),tf.linspace(0.0,d,d))\n",
    "    X = tf.reshape(X,[d,d,1])\n",
    "    Y = tf.reshape(Y,[d,d,1])\n",
    "    X_stack = tf.tile(X,tf.constant([1,1,17],dtype=tf.int32))\n",
    "    Y_stack = tf.tile(Y,tf.constant([1,1,17],dtype=tf.int32))\n",
    "\n",
    "    pts = tf.reshape(keypoints,[1,2,17])\n",
    "    ptsX, ptsY = tf.split(pts,[1,1],axis=1)\n",
    "    d1 = tf.square(tf.subtract(X_stack,ptsX))\n",
    "    d2 = tf.square(tf.subtract(Y_stack,ptsY))\n",
    "\n",
    "    pt_masks = tf.multiply(tf.divide(tf.constant(1.0),tf.add(d1,d2)+L),L)\n",
    "\n",
    "    return image, mask, pt_masks, pts, labels\n",
    "\n",
    "def generate_one_hot_keypoint_masks(image, mask, keypoints, labels, d=d):\n",
    "    pts = tf.reshape(keypoints,[1,2,17])\n",
    "    indices = tf.to_int32(pts)\n",
    "    kp_mask1 = tf.one_hot(depth=d,indices=indices[:,1,:],axis=0)\n",
    "    kp_mask2 = tf.one_hot(depth=d,indices=indices[:,0,:],axis=1)\n",
    "    kp_masks = tf.matmul(tf.transpose(kp_mask1,(2,0,1)),tf.transpose(kp_mask2,(2,0,1)))\n",
    "    kp_masks = tf.transpose(kp_masks,(1,2,0))\n",
    "    return image, mask, kp_masks, pts, labels\n",
    "\n",
    "\n",
    "#######################################################\n",
    "################## SUMMARY DICTIONARY #################\n",
    "#######################################################\n",
    "\n",
    "image_summary_list = []\n",
    "scalar_summary_list = []\n",
    "\n",
    "#######################################################\n",
    "################### PREPARE DATASET ###################\n",
    "#######################################################\n",
    "print(\"Initializing Dataset...\\n\")\n",
    "with tf.variable_scope('DataSet'):\n",
    "    ################### TRAIN DATASET ###################\n",
    "    train_filenames = tf.constant(['{}/COCO_train2014_{:0>12}.jpg'.format(train_img_path, imgID) for imgID in train_imgIds])\n",
    "    train_imgID_tensor = tf.constant(train_imgIds)\n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((train_filenames, train_imgID_tensor))\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda filename, imgID: tf.py_func(extract_annotations_train, [filename, imgID], [filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    train_dataset = train_dataset.map(preprocess_image_tf)\n",
    "    train_dataset = train_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    train_dataset = train_dataset.map(generate_one_hot_keypoint_masks)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    #################### VAL DATASET ####################\n",
    "    val_filenames = tf.constant(['{}/COCO_val2014_{:0>12}.jpg'.format(val_img_path, imgID) for imgID in val_imgIds])\n",
    "    val_imgID_tensor = tf.constant(val_imgIds)\n",
    "    val_dataset = tf.contrib.data.Dataset.from_tensor_slices((val_filenames, val_imgID_tensor))\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda filename, imgID: tf.py_func(extract_annotations_val,[filename, imgID],[filename.dtype, tf.int64, tf.int64, tf.uint8]))\n",
    "    val_dataset = val_dataset.map(preprocess_image_tf)\n",
    "    val_dataset = val_dataset.map(scaleDownMaskAndKeypoints)\n",
    "    val_dataset = val_dataset.map(generate_one_hot_keypoint_masks)\n",
    "    val_dataset = val_dataset.shuffle(buffer_size=10000)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    iterator = tf.contrib.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "\n",
    "    images, masks, kpt_masks, pts, labels = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(train_dataset)\n",
    "    val_init_op = iterator.make_initializer(val_dataset)\n",
    "\n",
    "    image_summary_list.append(tf.summary.image('keypoint_masks', getActivationImage(kpt_masks)))\n",
    "    image_summary_list.append(tf.summary.image('input_images', images))\n",
    "\n",
    "#######################################################\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_keypoint_masks(kpt_masks, d=56):\n",
    "    return tf.reshape(tf.nn.softmax(tf.reshape(kpt_masks, [-1,d**2,17]),dim=1),[-1,d,d,17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNetMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block1 = endpoints['resnet_v2_50/block1']\n",
    "block2 = endpoints['resnet_v2_50/block2']\n",
    "block3 = endpoints['resnet_v2_50/block3']\n",
    "block4 = endpoints['resnet_v2_50/block4']\n",
    "\n",
    "with tf.variable_scope('block3_head'):\n",
    "    # initialize as bilinear upsampling\n",
    "    input_shape=block3.shape.as_list()\n",
    "    w3 = bilinear_filter(input_shape[3],128)\n",
    "    b3 = tf.nn.conv2d_transpose(block3,w3,output_shape=tf.constant(\n",
    "            [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "    b3 = tf.layers.batch_normalization(b3,axis=3) # axis=3 --> IMPORTANT!!\n",
    "    b3 = tf.nn.relu(b3)\n",
    "with tf.variable_scope('block4_head'):\n",
    "    input_shape=block4.shape.as_list()\n",
    "    w4 = bilinear_filter(input_shape[3],128)\n",
    "    b4 = tf.nn.conv2d_transpose(block4,w4,output_shape=tf.constant(\n",
    "            [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "    b4 = tf.layers.batch_normalization(b4,axis=3) # axis=3 --> IMPORTANT!!\n",
    "    b4 = tf.nn.relu(b4)\n",
    "    \n",
    "net = tf.concat([b3,b4],axis=3)\n",
    "input_shape=net.shape.as_list()\n",
    "w = bilinear_filter(input_shape[3],128)\n",
    "net = tf.nn.conv2d_transpose(net,w,output_shape=tf.constant(\n",
    "        [input_shape[0],2*input_shape[1],2*input_shape[2],128]),strides=(2,2),padding='SAME')\n",
    "net = tf.layers.batch_normalization(net,axis=3) # axis=3 --> IMPORTANT!!\n",
    "net = tf.nn.relu(net)\n",
    "\n",
    "logits = tf.layers.conv2d(net,17,(1,1),(1,1),kernel_initializer=X_INIT)\n",
    "\n",
    "with tf.variable_scope('keypoint_predictions'):\n",
    "    keypoint_predictions = softmax_keypoint_masks(logits, d=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HourGlass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HourGlassNet(graph, num_levels=5, inputs, scope=None, scalar_summary_list=None, image_summary_list=None, histogram_summary_list=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - net \n",
    "    - output_points\n",
    "    - summary lists\n",
    "    \"\"\"\n",
    "    with graph.as_default():\n",
    "        # Initialize summaries\n",
    "        if scalar_summary_list is None:\n",
    "            scalar_summary_list = []\n",
    "        if image_summary_list is None:\n",
    "            image_summary_list = []\n",
    "        if histogram_summary_list is None:\n",
    "            histogram_summary_list = []\n",
    "            \n",
    "        head = {}\n",
    "        with tf.variable_scope('Hourglass'):\n",
    "            with tf.variable_scope('base'):\n",
    "                base = tf.layers.conv2d(inputs,64,(3,3),strides=(1,1),padding='SAME')\n",
    "                base = tf.layers.batch_normalization(base,axis=3)\n",
    "                base = tf.layers.relu(base)\n",
    "\n",
    "            for level in range(num_levels):\n",
    "                with tf.variable_scope('level_{}'.format(level+1)):\n",
    "                    base = tf.layers.conv2d(base,32*2**level,(3,3),strides=(2,2),padding='SAME')\n",
    "                    base = tf.layers.batch_normalization(base,axis=3)\n",
    "                    base = tf.layers.relu(base)\n",
    "\n",
    "                    output = base\n",
    "                    for i in range(num_levels - level):\n",
    "                        output = tf.layers.conv2d(output,32*2**level,(3,3),strides=(1,1),padding='SAME')\n",
    "                        output = tf.layers.batch_normalization(output,axis=3)\n",
    "                        output = tf.layers.relu(output)\n",
    "                    head[level] = output\n",
    "\n",
    "            for level in range(num_levels):\n",
    "                # resize_bilinear or upconv?\n",
    "                # output = tf.image.resize_bilinear(ouput,size=2*output.shape[1:2])\n",
    "                output = tf.layers.conv2d_transpose(output,32*2**level)\n",
    "\n",
    "                output = tf.layers.batch_normalization(output,axis=3) # HERE OR AFTER CONCAT???\n",
    "                output = tf.layers.relu(output) # HERE OR AFTER CONCAT???\n",
    "                output = tf.concat(head[level],output,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_levels = 5\n",
    "D = 224\n",
    "image = tf.placeholder(tf.float32,[1,D,D,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for level in range(num_levels):\n",
    "    d_bridge = int(D/(2**level))\n",
    "    filters_bridge = 64*2**level\n",
    "    d_base = int(d_bridge/2)\n",
    "    filters_base = filters_bridge * 2\n",
    "    stack_size = num_levels - level\n",
    "    print(level,'-- d_bridge: {:>3} x {:<3} x {:<4}'.format(d_bridge,d_bridge,filters_bridge),'--- {}'.format(stack_size))\n",
    "    print('       d_base: {:>3} x {:<3} x {:<4}'.format(d_base,d_base,filters_base),'\\n')\n",
    "    \n",
    "print(\"### BRIDGE ###\")\n",
    "    \n",
    "for level in reversed(range(num_levels)):\n",
    "    d_out = int(D/(2**level))\n",
    "    filters_out = 64*2**level\n",
    "    d_bridge = int(d_out/2)\n",
    "    filters_bridge = filters_out * 2\n",
    "    print(level,'-- d_bridge: {:>3} x {:<3} x {:<4}'.format(d_bridge,d_bridge,filters_bridge),'\\n')\n",
    "    print('        d_out: {:>3} x {:<3} x {:<4}'.format(d_out,d_out,filters_out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32,[1,224,224,3])\n",
    "num_levels = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = tf.layers.conv2d(inputs, 64, (3,3),strides=(1,1),padding='SAME')\n",
    "base = tf.layers.batch_normalization(base,axis=3)\n",
    "base = tf.nn.relu(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for level in range(num_levels):\n",
    "#     with tf.variable_scope('level_{}'.format(level+1)):\n",
    "    bridge_filters = 32*2**level\n",
    "    # Base - decrease size by factor of 2 for n\n",
    "    base = tf.layers.conv2d(base,bridge_filters,(3,3),strides=(2,2),padding='SAME')\n",
    "    base = tf.layers.batch_normalization(base,axis=3)\n",
    "    base = tf.nn.relu(base)\n",
    "\n",
    "    # Bridge - maintain constant size\n",
    "    bridge = base\n",
    "    for i in range(num_levels - level):\n",
    "        bridge = tf.layers.conv2d(bridge,bridge_filters,(3,3),strides=(1,1),padding='SAME')\n",
    "        bridge = tf.layers.batch_normalization(bridge,axis=3)\n",
    "        bridge = tf.nn.relu(bridge)\n",
    "    head[level] = bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <tf.Tensor 'concat_21:0' shape=(1, 112, 112, 64) dtype=float32>,\n",
       " 1: <tf.Tensor 'concat_20:0' shape=(1, 56, 56, 128) dtype=float32>,\n",
       " 2: <tf.Tensor 'concat_19:0' shape=(1, 28, 28, 256) dtype=float32>,\n",
       " 3: <tf.Tensor 'concat_18:0' shape=(1, 14, 14, 512) dtype=float32>,\n",
       " 4: <tf.Tensor 'Relu_125:0' shape=(1, 7, 7, 512) dtype=float32>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*2**(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for level in reversed(range(num_levels)):\n",
    "    # resize_bilinear or upconv?\n",
    "    # output = tf.image.resize_bilinear(ouput,size=2*output.shape[1:2])\n",
    "    out_filters = 16*2**(level)\n",
    "\n",
    "    output = tf.layers.conv2d_transpose(head[level],out_filters,(3,3),(2,2),padding='SAME')\n",
    "    output = tf.layers.batch_normalization(output,axis=3) # HERE OR AFTER CONCAT???\n",
    "    output = tf.nn.relu(output) # HERE OR AFTER CONCAT???\n",
    "\n",
    "    if level > 0:\n",
    "        head[level-1] = tf.concat([head[level-1],output],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    head = {}\n",
    "    inputs = tf.placeholder(tf.float32,[1,224,224,3])\n",
    "    num_levels = 5\n",
    "    base_filters = 64\n",
    "    with tf.variable_scope('Hourglass'):\n",
    "        with tf.variable_scope('base'):\n",
    "            base = tf.layers.conv2d(inputs, base_filters, (3,3),strides=(1,1),padding='SAME')\n",
    "            base = tf.layers.batch_normalization(base,axis=3)\n",
    "            base = tf.nn.relu(base)\n",
    "\n",
    "        for level in range(num_levels):\n",
    "            with tf.variable_scope('level_{}'.format(level+1)):\n",
    "                bridge_filters = base_filters*2**level\n",
    "                # Base - decrease size by factor of 2 for n\n",
    "                base = tf.layers.conv2d(base,bridge_filters,(3,3),strides=(2,2),padding='SAME')\n",
    "                base = tf.layers.batch_normalization(base,axis=3)\n",
    "                base = tf.nn.relu(base)\n",
    "\n",
    "                # Bridge - maintain constant size\n",
    "                bridge = base\n",
    "                for i in range(num_levels - level):\n",
    "                    bridge = tf.layers.conv2d(bridge,bridge_filters,(3,3),strides=(1,1),padding='SAME')\n",
    "                    bridge = tf.layers.batch_normalization(bridge,axis=3)\n",
    "                    bridge = tf.nn.relu(bridge)\n",
    "                head[level] = bridge\n",
    "\n",
    "        for level in reversed(range(num_levels)):\n",
    "            # resize_bilinear or upconv?\n",
    "            # output = tf.image.resize_bilinear(ouput,size=2*output.shape[1:2])\n",
    "            out_filters = (base_filters//2)*2**(level)\n",
    "\n",
    "            output = tf.layers.conv2d_transpose(head[level],out_filters,(3,3),(2,2),padding='SAME')\n",
    "            output = tf.layers.batch_normalization(output,axis=3) # HERE OR AFTER CONCAT???\n",
    "            output = tf.nn.relu(output) # HERE OR AFTER CONCAT???\n",
    "\n",
    "            if level > 0:\n",
    "                head[level-1] = tf.concat([head[level-1],output],axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
